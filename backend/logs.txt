
==> Audit <==
|------------|----------------|----------|-------|---------|-----------------------|-----------------------|
|  Command   |      Args      | Profile  | User  | Version |      Start Time       |       End Time        |
|------------|----------------|----------|-------|---------|-----------------------|-----------------------|
| start      |                | minikube | gihan | v1.33.0 | 05 May 24 10:10 +0530 | 05 May 24 10:19 +0530 |
| docker-env |                | minikube | gihan | v1.33.0 | 05 May 24 10:33 +0530 | 05 May 24 10:33 +0530 |
| docker-env |                | minikube | gihan | v1.33.0 | 05 May 24 10:36 +0530 | 05 May 24 10:36 +0530 |
| tunnel     |                | minikube | gihan | v1.33.0 | 05 May 24 14:11 +0530 | 05 May 24 14:15 +0530 |
| tunnel     |                | minikube | gihan | v1.33.0 | 05 May 24 14:15 +0530 | 05 May 24 14:17 +0530 |
| tunnel     |                | minikube | gihan | v1.33.0 | 05 May 24 14:17 +0530 | 05 May 24 14:25 +0530 |
| addons     | enable ingress | minikube | gihan | v1.33.0 | 05 May 24 14:25 +0530 |                       |
| addons     | enable ingress | minikube | gihan | v1.33.0 | 05 May 24 14:28 +0530 |                       |
| ip         |                | minikube | gihan | v1.33.0 | 05 May 24 14:29 +0530 | 05 May 24 14:29 +0530 |
|------------|----------------|----------|-------|---------|-----------------------|-----------------------|


==> Last Start <==
Log file created at: 2024/05/05 10:10:10
Running on machine: Gihans-MacBook-Air
Binary: Built with gc go1.22.2 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0505 10:10:10.834717   52118 out.go:291] Setting OutFile to fd 1 ...
I0505 10:10:10.835298   52118 out.go:343] isatty.IsTerminal(1) = true
I0505 10:10:10.835300   52118 out.go:304] Setting ErrFile to fd 2...
I0505 10:10:10.835303   52118 out.go:343] isatty.IsTerminal(2) = true
I0505 10:10:10.835425   52118 root.go:338] Updating PATH: /Users/gihan/.minikube/bin
W0505 10:10:10.835511   52118 root.go:314] Error reading config file at /Users/gihan/.minikube/config/config.json: open /Users/gihan/.minikube/config/config.json: no such file or directory
I0505 10:10:10.837083   52118 out.go:298] Setting JSON to false
I0505 10:10:10.866434   52118 start.go:129] hostinfo: {"hostname":"Gihans-MacBook-Air.local","uptime":475763,"bootTime":1714408247,"procs":473,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"14.3.1","kernelVersion":"23.3.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"ba738e4f-ba11-5000-9507-40ada4cfa805"}
W0505 10:10:10.866523   52118 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0505 10:10:10.872916   52118 out.go:177] 😄  minikube v1.33.0 on Darwin 14.3.1 (arm64)
W0505 10:10:10.889063   52118 preload.go:294] Failed to list preload files: open /Users/gihan/.minikube/cache/preloaded-tarball: no such file or directory
I0505 10:10:10.889073   52118 notify.go:220] Checking for updates...
I0505 10:10:10.889356   52118 driver.go:392] Setting default libvirt URI to qemu:///system
I0505 10:10:10.889396   52118 global.go:112] Querying for installed drivers using PATH=/Users/gihan/.minikube/bin:/Users/gihan/.console-ninja/.bin:/Library/Frameworks/Python.framework/Versions/3.11/bin:/Users/gihan/.nvm/versions/node/v20.10.0/bin:/Users/gihan/.bun/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/gihan/.console-ninja/.bin:/Library/Frameworks/Python.framework/Versions/3.11/bin:/Users/gihan/.nvm/versions/node/v20.10.0/bin:/Users/gihan/.bun/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/Users/gihan/.docker/bin.:/Users/gihan/Library/Application Support/JetBrains/Toolbox/scripts:/Users/gihan/.docker/bin.:/Users/gihan/Library/Application Support/JetBrains/Toolbox/scripts
I0505 10:10:10.889605   52118 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0505 10:10:10.889748   52118 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0505 10:10:11.159437   52118 docker.go:122] docker version: linux-25.0.3:Docker Desktop 4.27.2 (137060)
I0505 10:10:11.159629   52118 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0505 10:10:11.466185   52118 info.go:266] docker info: {ID:c794ec65-3d22-4c1e-9960-367de2dc6e5c Containers:5 ContainersRunning:0 ContainersPaused:0 ContainersStopped:5 Images:6 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:46 OomKillDisable:false NGoroutines:84 SystemTime:2024-05-05 04:40:11.448768347 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.6.12-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4113825792 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/gihan/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:/Users/gihan/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.5-desktop.1] map[Name:debug Path:/Users/gihan/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.24] map[Name:dev Path:/Users/gihan/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/gihan/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:/Users/gihan/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:/Users/gihan/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.0] map[Name:sbom Path:/Users/gihan/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/gihan/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.4.1]] Warnings:<nil>}}
I0505 10:10:11.466281   52118 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0505 10:10:11.466483   52118 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0505 10:10:11.466492   52118 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0505 10:10:11.466593   52118 global.go:133] hyperkit default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "hyperkit": executable file not found in $PATH Reason: Fix:Run 'brew install hyperkit' Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/hyperkit/ Version:}
I0505 10:10:11.466649   52118 global.go:133] parallels default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "prlctl": executable file not found in $PATH Reason: Fix:Install Parallels Desktop for Mac Doc:https://minikube.sigs.k8s.io/docs/drivers/parallels/ Version:}
I0505 10:10:11.466734   52118 global.go:133] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-aarch64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0505 10:10:11.466742   52118 driver.go:314] not recommending "ssh" due to default: false
I0505 10:10:11.466760   52118 driver.go:349] Picked: docker
I0505 10:10:11.466763   52118 driver.go:350] Alternatives: [ssh]
I0505 10:10:11.466766   52118 driver.go:351] Rejects: [virtualbox vmware podman hyperkit parallels qemu2]
I0505 10:10:11.474954   52118 out.go:177] ✨  Automatically selected the docker driver
I0505 10:10:11.477903   52118 start.go:297] selected driver: docker
I0505 10:10:11.477905   52118 start.go:901] validating driver "docker" against <nil>
I0505 10:10:11.477911   52118 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0505 10:10:11.477991   52118 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0505 10:10:11.586992   52118 info.go:266] docker info: {ID:c794ec65-3d22-4c1e-9960-367de2dc6e5c Containers:5 ContainersRunning:0 ContainersPaused:0 ContainersStopped:5 Images:6 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:46 OomKillDisable:false NGoroutines:84 SystemTime:2024-05-05 04:40:11.571465888 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.6.12-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4113825792 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/gihan/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:/Users/gihan/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.5-desktop.1] map[Name:debug Path:/Users/gihan/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.24] map[Name:dev Path:/Users/gihan/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/gihan/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:/Users/gihan/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:/Users/gihan/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.0] map[Name:sbom Path:/Users/gihan/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/gihan/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.4.1]] Warnings:<nil>}}
I0505 10:10:11.587138   52118 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0505 10:10:11.587246   52118 start_flags.go:393] Using suggested 2200MB memory alloc based on sys=8192MB, container=3923MB
I0505 10:10:11.587974   52118 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0505 10:10:11.592880   52118 out.go:177] 📌  Using Docker Desktop driver with root privileges
I0505 10:10:11.597911   52118 cni.go:84] Creating CNI manager for ""
I0505 10:10:11.597930   52118 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0505 10:10:11.597933   52118 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0505 10:10:11.597987   52118 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0505 10:10:11.601880   52118 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0505 10:10:11.605887   52118 cache.go:121] Beginning downloading kic base image for docker with docker
I0505 10:10:11.610849   52118 out.go:177] 🚜  Pulling base image v0.0.43 ...
I0505 10:10:11.618109   52118 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0505 10:10:11.618175   52118 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon
I0505 10:10:11.664891   52118 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 to local cache
I0505 10:10:11.665511   52118 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local cache directory
I0505 10:10:11.665626   52118 image.go:118] Writing gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 to local cache
I0505 10:10:12.810963   52118 preload.go:119] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.30.0/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4
I0505 10:10:12.810984   52118 cache.go:56] Caching tarball of preloaded images
I0505 10:10:12.811354   52118 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0505 10:10:12.816621   52118 out.go:177] 💾  Downloading Kubernetes v1.30.0 preload ...
I0505 10:10:12.823687   52118 preload.go:237] getting checksum for preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4 ...
I0505 10:10:14.321207   52118 download.go:107] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.30.0/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4?checksum=md5:677034533668c42fec962cc52f9b3c42 -> /Users/gihan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4
I0505 10:15:34.228011   52118 preload.go:248] saving checksum for preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4 ...
I0505 10:15:34.228182   52118 preload.go:255] verifying checksum of /Users/gihan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4 ...
I0505 10:15:34.847899   52118 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0505 10:15:34.849832   52118 profile.go:143] Saving config to /Users/gihan/.minikube/profiles/minikube/config.json ...
I0505 10:15:34.849864   52118 lock.go:35] WriteFile acquiring /Users/gihan/.minikube/profiles/minikube/config.json: {Name:mk095a5430b63eed067c9a4b7adfb40cd6efc6ae Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0505 10:18:43.956094   52118 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 as a tarball
I0505 10:18:43.956594   52118 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 from local cache
I0505 10:19:04.610001   52118 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 from cached tarball
I0505 10:19:04.610834   52118 cache.go:194] Successfully downloaded all kic artifacts
I0505 10:19:04.615015   52118 start.go:360] acquireMachinesLock for minikube: {Name:mke9f8e4eff4de2241a5c366a4f8edec6b2fbf25 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0505 10:19:04.619349   52118 start.go:364] duration metric: took 3.513584ms to acquireMachinesLock for "minikube"
I0505 10:19:04.620006   52118 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0505 10:19:04.624530   52118 start.go:125] createHost starting for "" (driver="docker")
I0505 10:19:04.643769   52118 out.go:204] 🔥  Creating docker container (CPUs=2, Memory=2200MB) ...
I0505 10:19:04.649934   52118 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0505 10:19:04.650892   52118 client.go:168] LocalClient.Create starting
I0505 10:19:04.653750   52118 main.go:141] libmachine: Creating CA: /Users/gihan/.minikube/certs/ca.pem
I0505 10:19:04.884237   52118 main.go:141] libmachine: Creating client certificate: /Users/gihan/.minikube/certs/cert.pem
I0505 10:19:04.993412   52118 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0505 10:19:05.053042   52118 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0505 10:19:05.053161   52118 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I0505 10:19:05.053185   52118 cli_runner.go:164] Run: docker network inspect minikube
W0505 10:19:05.107142   52118 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0505 10:19:05.107190   52118 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0505 10:19:05.107252   52118 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0505 10:19:05.108464   52118 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0505 10:19:05.158578   52118 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0x140030140a0}
I0505 10:19:05.159276   52118 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 65535 ...
I0505 10:19:05.159678   52118 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=65535 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0505 10:19:05.249676   52118 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0505 10:19:05.250803   52118 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0505 10:19:05.251533   52118 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0505 10:19:05.296436   52118 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0505 10:19:05.347067   52118 oci.go:103] Successfully created a docker volume minikube
I0505 10:19:05.347400   52118 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -d /var/lib
I0505 10:19:06.190195   52118 oci.go:107] Successfully prepared a docker volume minikube
I0505 10:19:06.191241   52118 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0505 10:19:06.192669   52118 kic.go:194] Starting extracting preloaded images to volume ...
I0505 10:19:06.193510   52118 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/gihan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir
I0505 10:19:09.346979   52118 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/gihan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir: (3.153229208s)
I0505 10:19:09.347124   52118 kic.go:203] duration metric: took 3.154447916s to extract preloaded images to volume ...
I0505 10:19:09.348613   52118 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0505 10:19:09.924408   52118 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737
I0505 10:19:10.369399   52118 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0505 10:19:10.429290   52118 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0505 10:19:10.484399   52118 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0505 10:19:10.604682   52118 oci.go:144] the created container "minikube" has a running status.
I0505 10:19:10.605110   52118 kic.go:225] Creating ssh key for kic: /Users/gihan/.minikube/machines/minikube/id_rsa...
I0505 10:19:10.712020   52118 kic_runner.go:191] docker (temp): /Users/gihan/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0505 10:19:10.780325   52118 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0505 10:19:10.843096   52118 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0505 10:19:10.843115   52118 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0505 10:19:10.932144   52118 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0505 10:19:10.995156   52118 machine.go:94] provisionDockerMachine start ...
I0505 10:19:10.996432   52118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0505 10:19:11.048393   52118 main.go:141] libmachine: Using SSH client type: native
I0505 10:19:11.051541   52118 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1048c3280] 0x1048c5ae0 <nil>  [] 0s} 127.0.0.1 58254 <nil> <nil>}
I0505 10:19:11.051560   52118 main.go:141] libmachine: About to run SSH command:
hostname
I0505 10:19:11.221218   52118 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0505 10:19:11.221989   52118 ubuntu.go:169] provisioning hostname "minikube"
I0505 10:19:11.223210   52118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0505 10:19:11.285659   52118 main.go:141] libmachine: Using SSH client type: native
I0505 10:19:11.287039   52118 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1048c3280] 0x1048c5ae0 <nil>  [] 0s} 127.0.0.1 58254 <nil> <nil>}
I0505 10:19:11.287047   52118 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0505 10:19:11.454825   52118 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0505 10:19:11.455964   52118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0505 10:19:11.508315   52118 main.go:141] libmachine: Using SSH client type: native
I0505 10:19:11.508495   52118 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1048c3280] 0x1048c5ae0 <nil>  [] 0s} 127.0.0.1 58254 <nil> <nil>}
I0505 10:19:11.508501   52118 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0505 10:19:11.637944   52118 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0505 10:19:11.637963   52118 ubuntu.go:175] set auth options {CertDir:/Users/gihan/.minikube CaCertPath:/Users/gihan/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/gihan/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/gihan/.minikube/machines/server.pem ServerKeyPath:/Users/gihan/.minikube/machines/server-key.pem ClientKeyPath:/Users/gihan/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/gihan/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/gihan/.minikube}
I0505 10:19:11.639738   52118 ubuntu.go:177] setting up certificates
I0505 10:19:11.640894   52118 provision.go:84] configureAuth start
I0505 10:19:11.643848   52118 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0505 10:19:11.709826   52118 provision.go:143] copyHostCerts
I0505 10:19:11.712060   52118 exec_runner.go:151] cp: /Users/gihan/.minikube/certs/key.pem --> /Users/gihan/.minikube/key.pem (1679 bytes)
I0505 10:19:11.713302   52118 exec_runner.go:151] cp: /Users/gihan/.minikube/certs/ca.pem --> /Users/gihan/.minikube/ca.pem (1074 bytes)
I0505 10:19:11.714930   52118 exec_runner.go:151] cp: /Users/gihan/.minikube/certs/cert.pem --> /Users/gihan/.minikube/cert.pem (1119 bytes)
I0505 10:19:11.724348   52118 provision.go:117] generating server cert: /Users/gihan/.minikube/machines/server.pem ca-key=/Users/gihan/.minikube/certs/ca.pem private-key=/Users/gihan/.minikube/certs/ca-key.pem org=gihan.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0505 10:19:11.824102   52118 provision.go:177] copyRemoteCerts
I0505 10:19:11.833260   52118 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0505 10:19:11.833358   52118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0505 10:19:11.890927   52118 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58254 SSHKeyPath:/Users/gihan/.minikube/machines/minikube/id_rsa Username:docker}
I0505 10:19:11.989006   52118 ssh_runner.go:362] scp /Users/gihan/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0505 10:19:12.020000   52118 ssh_runner.go:362] scp /Users/gihan/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0505 10:19:12.046879   52118 ssh_runner.go:362] scp /Users/gihan/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0505 10:19:12.067889   52118 provision.go:87] duration metric: took 426.256459ms to configureAuth
I0505 10:19:12.068463   52118 ubuntu.go:193] setting minikube options for container-runtime
I0505 10:19:12.077935   52118 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0505 10:19:12.078012   52118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0505 10:19:12.126371   52118 main.go:141] libmachine: Using SSH client type: native
I0505 10:19:12.131243   52118 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1048c3280] 0x1048c5ae0 <nil>  [] 0s} 127.0.0.1 58254 <nil> <nil>}
I0505 10:19:12.131251   52118 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0505 10:19:12.266047   52118 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0505 10:19:12.266057   52118 ubuntu.go:71] root file system type: overlay
I0505 10:19:12.271959   52118 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0505 10:19:12.272056   52118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0505 10:19:12.327584   52118 main.go:141] libmachine: Using SSH client type: native
I0505 10:19:12.327765   52118 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1048c3280] 0x1048c5ae0 <nil>  [] 0s} 127.0.0.1 58254 <nil> <nil>}
I0505 10:19:12.327796   52118 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0505 10:19:12.486655   52118 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0505 10:19:12.487441   52118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0505 10:19:12.588004   52118 main.go:141] libmachine: Using SSH client type: native
I0505 10:19:12.588357   52118 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1048c3280] 0x1048c5ae0 <nil>  [] 0s} 127.0.0.1 58254 <nil> <nil>}
I0505 10:19:12.588366   52118 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0505 10:19:13.528614   52118 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-04-11 10:51:51.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-05-05 04:49:12.482109010 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0505 10:19:13.528653   52118 machine.go:97] duration metric: took 2.533425125s to provisionDockerMachine
I0505 10:19:13.529846   52118 client.go:171] duration metric: took 8.878772708s to LocalClient.Create
I0505 10:19:13.530336   52118 start.go:167] duration metric: took 8.880392917s to libmachine.API.Create "minikube"
I0505 10:19:13.530750   52118 start.go:293] postStartSetup for "minikube" (driver="docker")
I0505 10:19:13.531136   52118 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0505 10:19:13.531637   52118 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0505 10:19:13.531679   52118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0505 10:19:13.582474   52118 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58254 SSHKeyPath:/Users/gihan/.minikube/machines/minikube/id_rsa Username:docker}
I0505 10:19:13.675532   52118 ssh_runner.go:195] Run: cat /etc/os-release
I0505 10:19:13.678362   52118 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0505 10:19:13.678382   52118 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0505 10:19:13.678386   52118 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0505 10:19:13.678389   52118 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0505 10:19:13.678890   52118 filesync.go:126] Scanning /Users/gihan/.minikube/addons for local assets ...
I0505 10:19:13.679059   52118 filesync.go:126] Scanning /Users/gihan/.minikube/files for local assets ...
I0505 10:19:13.679099   52118 start.go:296] duration metric: took 148.336542ms for postStartSetup
I0505 10:19:13.680207   52118 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0505 10:19:13.723153   52118 profile.go:143] Saving config to /Users/gihan/.minikube/profiles/minikube/config.json ...
I0505 10:19:13.724244   52118 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0505 10:19:13.724289   52118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0505 10:19:13.768647   52118 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58254 SSHKeyPath:/Users/gihan/.minikube/machines/minikube/id_rsa Username:docker}
I0505 10:19:13.860649   52118 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0505 10:19:13.864026   52118 start.go:128] duration metric: took 9.239094042s to createHost
I0505 10:19:13.864197   52118 start.go:83] releasing machines lock for "minikube", held for 9.244672459s
I0505 10:19:13.864591   52118 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0505 10:19:13.909164   52118 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0505 10:19:13.909391   52118 ssh_runner.go:195] Run: cat /version.json
I0505 10:19:13.909447   52118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0505 10:19:13.910438   52118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0505 10:19:13.954194   52118 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58254 SSHKeyPath:/Users/gihan/.minikube/machines/minikube/id_rsa Username:docker}
I0505 10:19:13.954344   52118 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58254 SSHKeyPath:/Users/gihan/.minikube/machines/minikube/id_rsa Username:docker}
I0505 10:19:14.477732   52118 ssh_runner.go:195] Run: systemctl --version
I0505 10:19:14.483027   52118 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0505 10:19:14.486015   52118 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0505 10:19:14.510813   52118 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0505 10:19:14.511177   52118 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0505 10:19:14.537531   52118 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0505 10:19:14.537846   52118 start.go:494] detecting cgroup driver to use...
I0505 10:19:14.537876   52118 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0505 10:19:14.538491   52118 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0505 10:19:14.555790   52118 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0505 10:19:14.577901   52118 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0505 10:19:14.590457   52118 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0505 10:19:14.590615   52118 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0505 10:19:14.599603   52118 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0505 10:19:14.607914   52118 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0505 10:19:14.619385   52118 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0505 10:19:14.628526   52118 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0505 10:19:14.637935   52118 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0505 10:19:14.647386   52118 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0505 10:19:14.656566   52118 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0505 10:19:14.664955   52118 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0505 10:19:14.673848   52118 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0505 10:19:14.682083   52118 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0505 10:19:14.722425   52118 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0505 10:19:14.797021   52118 start.go:494] detecting cgroup driver to use...
I0505 10:19:14.797048   52118 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0505 10:19:14.797205   52118 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0505 10:19:14.816634   52118 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0505 10:19:14.817008   52118 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0505 10:19:14.831366   52118 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0505 10:19:14.853416   52118 ssh_runner.go:195] Run: which cri-dockerd
I0505 10:19:14.858755   52118 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0505 10:19:14.867336   52118 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0505 10:19:14.882797   52118 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0505 10:19:14.947679   52118 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0505 10:19:14.996855   52118 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0505 10:19:15.030793   52118 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0505 10:19:15.047583   52118 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0505 10:19:15.087595   52118 ssh_runner.go:195] Run: sudo systemctl restart docker
I0505 10:19:15.626218   52118 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0505 10:19:15.640850   52118 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0505 10:19:15.652346   52118 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0505 10:19:15.695964   52118 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0505 10:19:15.735831   52118 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0505 10:19:15.778022   52118 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0505 10:19:15.805909   52118 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0505 10:19:15.816600   52118 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0505 10:19:15.995672   52118 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0505 10:19:16.097098   52118 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0505 10:19:16.098651   52118 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0505 10:19:16.101613   52118 start.go:562] Will wait 60s for crictl version
I0505 10:19:16.101776   52118 ssh_runner.go:195] Run: which crictl
I0505 10:19:16.103904   52118 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0505 10:19:16.135001   52118 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.0.1
RuntimeApiVersion:  v1
I0505 10:19:16.135136   52118 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0505 10:19:16.151925   52118 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0505 10:19:16.167443   52118 out.go:204] 🐳  Preparing Kubernetes v1.30.0 on Docker 26.0.1 ...
I0505 10:19:16.168907   52118 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0505 10:19:16.371302   52118 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0505 10:19:16.372520   52118 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0505 10:19:16.376345   52118 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0505 10:19:16.389159   52118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0505 10:19:16.434654   52118 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0505 10:19:16.435401   52118 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0505 10:19:16.435626   52118 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0505 10:19:16.455923   52118 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0505 10:19:16.455945   52118 docker.go:615] Images already preloaded, skipping extraction
I0505 10:19:16.456379   52118 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0505 10:19:16.467011   52118 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0505 10:19:16.467023   52118 cache_images.go:84] Images are preloaded, skipping loading
I0505 10:19:16.467030   52118 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0505 10:19:16.467791   52118 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0505 10:19:16.468000   52118 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0505 10:19:16.506096   52118 cni.go:84] Creating CNI manager for ""
I0505 10:19:16.506111   52118 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0505 10:19:16.506293   52118 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0505 10:19:16.506304   52118 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0505 10:19:16.506759   52118 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0505 10:19:16.506870   52118 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0505 10:19:16.515750   52118 binaries.go:44] Found k8s binaries, skipping transfer
I0505 10:19:16.515835   52118 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0505 10:19:16.524332   52118 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0505 10:19:16.541527   52118 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0505 10:19:16.558258   52118 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0505 10:19:16.575187   52118 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0505 10:19:16.577886   52118 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0505 10:19:16.586707   52118 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0505 10:19:16.629633   52118 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0505 10:19:16.662487   52118 certs.go:68] Setting up /Users/gihan/.minikube/profiles/minikube for IP: 192.168.49.2
I0505 10:19:16.662496   52118 certs.go:194] generating shared ca certs ...
I0505 10:19:16.662506   52118 certs.go:226] acquiring lock for ca certs: {Name:mk995a4af07581778e9675d856c556a3df12b464 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0505 10:19:16.664179   52118 certs.go:240] generating "minikubeCA" ca cert: /Users/gihan/.minikube/ca.key
I0505 10:19:16.812911   52118 crypto.go:156] Writing cert to /Users/gihan/.minikube/ca.crt ...
I0505 10:19:16.812926   52118 lock.go:35] WriteFile acquiring /Users/gihan/.minikube/ca.crt: {Name:mk8adb5dbb72361a077a816265b59903f8bb86be Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0505 10:19:16.813223   52118 crypto.go:164] Writing key to /Users/gihan/.minikube/ca.key ...
I0505 10:19:16.813226   52118 lock.go:35] WriteFile acquiring /Users/gihan/.minikube/ca.key: {Name:mk6ee5c3843e164d1574dedca65fd8ce059248aa Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0505 10:19:16.813415   52118 certs.go:240] generating "proxyClientCA" ca cert: /Users/gihan/.minikube/proxy-client-ca.key
I0505 10:19:17.054386   52118 crypto.go:156] Writing cert to /Users/gihan/.minikube/proxy-client-ca.crt ...
I0505 10:19:17.054394   52118 lock.go:35] WriteFile acquiring /Users/gihan/.minikube/proxy-client-ca.crt: {Name:mk5b934413f2e546ea6697bdecc435e7454fc416 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0505 10:19:17.054667   52118 crypto.go:164] Writing key to /Users/gihan/.minikube/proxy-client-ca.key ...
I0505 10:19:17.054669   52118 lock.go:35] WriteFile acquiring /Users/gihan/.minikube/proxy-client-ca.key: {Name:mkd7c2d9fbdd1f3c17ff6c2524ca7b7956417bc8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0505 10:19:17.054786   52118 certs.go:256] generating profile certs ...
I0505 10:19:17.054825   52118 certs.go:363] generating signed profile cert for "minikube-user": /Users/gihan/.minikube/profiles/minikube/client.key
I0505 10:19:17.055031   52118 crypto.go:68] Generating cert /Users/gihan/.minikube/profiles/minikube/client.crt with IP's: []
I0505 10:19:17.191872   52118 crypto.go:156] Writing cert to /Users/gihan/.minikube/profiles/minikube/client.crt ...
I0505 10:19:17.191884   52118 lock.go:35] WriteFile acquiring /Users/gihan/.minikube/profiles/minikube/client.crt: {Name:mk635bcfc18ac30e504905c17228dea2de74717d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0505 10:19:17.192172   52118 crypto.go:164] Writing key to /Users/gihan/.minikube/profiles/minikube/client.key ...
I0505 10:19:17.192185   52118 lock.go:35] WriteFile acquiring /Users/gihan/.minikube/profiles/minikube/client.key: {Name:mka3ab68188a391606664c17a84be9e109975dd7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0505 10:19:17.192411   52118 certs.go:363] generating signed profile cert for "minikube": /Users/gihan/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0505 10:19:17.192424   52118 crypto.go:68] Generating cert /Users/gihan/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0505 10:19:17.371158   52118 crypto.go:156] Writing cert to /Users/gihan/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0505 10:19:17.371167   52118 lock.go:35] WriteFile acquiring /Users/gihan/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk2858f85cafd312618e48a584ab9fb648610bd1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0505 10:19:17.371403   52118 crypto.go:164] Writing key to /Users/gihan/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0505 10:19:17.371406   52118 lock.go:35] WriteFile acquiring /Users/gihan/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk5bd654304febb3074bc54c1cf8a70111987c96 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0505 10:19:17.371533   52118 certs.go:381] copying /Users/gihan/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /Users/gihan/.minikube/profiles/minikube/apiserver.crt
I0505 10:19:17.371803   52118 certs.go:385] copying /Users/gihan/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /Users/gihan/.minikube/profiles/minikube/apiserver.key
I0505 10:19:17.372065   52118 certs.go:363] generating signed profile cert for "aggregator": /Users/gihan/.minikube/profiles/minikube/proxy-client.key
I0505 10:19:17.372078   52118 crypto.go:68] Generating cert /Users/gihan/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0505 10:19:17.579622   52118 crypto.go:156] Writing cert to /Users/gihan/.minikube/profiles/minikube/proxy-client.crt ...
I0505 10:19:17.579635   52118 lock.go:35] WriteFile acquiring /Users/gihan/.minikube/profiles/minikube/proxy-client.crt: {Name:mkc4efcf12cb9d8cd0cb326342df61c67131f3b9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0505 10:19:17.580933   52118 crypto.go:164] Writing key to /Users/gihan/.minikube/profiles/minikube/proxy-client.key ...
I0505 10:19:17.580937   52118 lock.go:35] WriteFile acquiring /Users/gihan/.minikube/profiles/minikube/proxy-client.key: {Name:mk0e3bd7231f5449d95f884c28b331b2de6870e3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0505 10:19:17.582312   52118 certs.go:484] found cert: /Users/gihan/.minikube/certs/ca-key.pem (1675 bytes)
I0505 10:19:17.582459   52118 certs.go:484] found cert: /Users/gihan/.minikube/certs/ca.pem (1074 bytes)
I0505 10:19:17.582551   52118 certs.go:484] found cert: /Users/gihan/.minikube/certs/cert.pem (1119 bytes)
I0505 10:19:17.582681   52118 certs.go:484] found cert: /Users/gihan/.minikube/certs/key.pem (1679 bytes)
I0505 10:19:17.595896   52118 ssh_runner.go:362] scp /Users/gihan/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0505 10:19:17.659885   52118 ssh_runner.go:362] scp /Users/gihan/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0505 10:19:17.696973   52118 ssh_runner.go:362] scp /Users/gihan/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0505 10:19:17.751761   52118 ssh_runner.go:362] scp /Users/gihan/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0505 10:19:17.780763   52118 ssh_runner.go:362] scp /Users/gihan/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0505 10:19:17.833924   52118 ssh_runner.go:362] scp /Users/gihan/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0505 10:19:17.866764   52118 ssh_runner.go:362] scp /Users/gihan/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0505 10:19:17.905110   52118 ssh_runner.go:362] scp /Users/gihan/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0505 10:19:17.953331   52118 ssh_runner.go:362] scp /Users/gihan/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0505 10:19:18.013260   52118 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (742 bytes)
I0505 10:19:18.050430   52118 ssh_runner.go:195] Run: openssl version
I0505 10:19:18.057225   52118 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0505 10:19:18.076639   52118 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0505 10:19:18.090677   52118 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May  5 04:49 /usr/share/ca-certificates/minikubeCA.pem
I0505 10:19:18.090798   52118 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0505 10:19:18.100611   52118 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0505 10:19:18.112672   52118 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0505 10:19:18.124805   52118 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0505 10:19:18.126849   52118 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0505 10:19:18.127106   52118 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0505 10:19:18.158133   52118 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0505 10:19:18.168528   52118 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0505 10:19:18.183557   52118 kubeadm.go:213] ignoring SystemVerification for kubeadm because of docker driver
I0505 10:19:18.183984   52118 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0505 10:19:18.216862   52118 kubeadm.go:154] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0505 10:19:18.216890   52118 kubeadm.go:156] found existing configuration files:

I0505 10:19:18.217045   52118 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0505 10:19:18.235773   52118 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0505 10:19:18.235931   52118 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0505 10:19:18.250129   52118 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0505 10:19:18.268664   52118 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0505 10:19:18.268817   52118 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0505 10:19:18.283093   52118 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0505 10:19:18.294155   52118 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0505 10:19:18.294519   52118 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0505 10:19:18.313206   52118 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0505 10:19:18.326695   52118 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0505 10:19:18.326850   52118 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0505 10:19:18.343495   52118 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0505 10:19:18.543370   52118 kubeadm.go:309] [init] Using Kubernetes version: v1.30.0
I0505 10:19:18.543462   52118 kubeadm.go:309] [preflight] Running pre-flight checks
I0505 10:19:18.688087   52118 kubeadm.go:309] [preflight] Pulling images required for setting up a Kubernetes cluster
I0505 10:19:18.688245   52118 kubeadm.go:309] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0505 10:19:18.688394   52118 kubeadm.go:309] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0505 10:19:18.906118   52118 kubeadm.go:309] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0505 10:19:18.926905   52118 out.go:204]     ▪ Generating certificates and keys ...
I0505 10:19:18.927077   52118 kubeadm.go:309] [certs] Using existing ca certificate authority
I0505 10:19:18.927164   52118 kubeadm.go:309] [certs] Using existing apiserver certificate and key on disk
I0505 10:19:19.207865   52118 kubeadm.go:309] [certs] Generating "apiserver-kubelet-client" certificate and key
I0505 10:19:19.314853   52118 kubeadm.go:309] [certs] Generating "front-proxy-ca" certificate and key
I0505 10:19:19.493957   52118 kubeadm.go:309] [certs] Generating "front-proxy-client" certificate and key
I0505 10:19:19.625115   52118 kubeadm.go:309] [certs] Generating "etcd/ca" certificate and key
I0505 10:19:19.758554   52118 kubeadm.go:309] [certs] Generating "etcd/server" certificate and key
I0505 10:19:19.758743   52118 kubeadm.go:309] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0505 10:19:19.800884   52118 kubeadm.go:309] [certs] Generating "etcd/peer" certificate and key
I0505 10:19:19.801877   52118 kubeadm.go:309] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0505 10:19:19.926278   52118 kubeadm.go:309] [certs] Generating "etcd/healthcheck-client" certificate and key
I0505 10:19:20.243064   52118 kubeadm.go:309] [certs] Generating "apiserver-etcd-client" certificate and key
I0505 10:19:20.711579   52118 kubeadm.go:309] [certs] Generating "sa" key and public key
I0505 10:19:20.711767   52118 kubeadm.go:309] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0505 10:19:20.870714   52118 kubeadm.go:309] [kubeconfig] Writing "admin.conf" kubeconfig file
I0505 10:19:21.008515   52118 kubeadm.go:309] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0505 10:19:21.185310   52118 kubeadm.go:309] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0505 10:19:21.252380   52118 kubeadm.go:309] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0505 10:19:21.449672   52118 kubeadm.go:309] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0505 10:19:21.449792   52118 kubeadm.go:309] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0505 10:19:21.459108   52118 kubeadm.go:309] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0505 10:19:21.774540   52118 out.go:204]     ▪ Booting up control plane ...
I0505 10:19:21.775034   52118 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0505 10:19:21.775130   52118 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0505 10:19:21.775204   52118 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0505 10:19:21.775346   52118 kubeadm.go:309] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0505 10:19:21.775477   52118 kubeadm.go:309] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0505 10:19:21.775554   52118 kubeadm.go:309] [kubelet-start] Starting the kubelet
I0505 10:19:21.775776   52118 kubeadm.go:309] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0505 10:19:21.776186   52118 kubeadm.go:309] [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
I0505 10:19:22.618481   52118 kubeadm.go:309] [kubelet-check] The kubelet is healthy after 1.008215167s
I0505 10:19:22.618785   52118 kubeadm.go:309] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0505 10:19:29.117915   52118 kubeadm.go:309] [api-check] The API server is healthy after 6.502924378s
I0505 10:19:29.131030   52118 kubeadm.go:309] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0505 10:19:29.138553   52118 kubeadm.go:309] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0505 10:19:29.166736   52118 kubeadm.go:309] [upload-certs] Skipping phase. Please see --upload-certs
I0505 10:19:29.167817   52118 kubeadm.go:309] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0505 10:19:29.176617   52118 kubeadm.go:309] [bootstrap-token] Using token: pterr3.zljc46roqm4whle9
I0505 10:19:29.183458   52118 out.go:204]     ▪ Configuring RBAC rules ...
I0505 10:19:29.183664   52118 kubeadm.go:309] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0505 10:19:29.186635   52118 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0505 10:19:29.192736   52118 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0505 10:19:29.194171   52118 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0505 10:19:29.196172   52118 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0505 10:19:29.197767   52118 kubeadm.go:309] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0505 10:19:29.528259   52118 kubeadm.go:309] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0505 10:19:29.972764   52118 kubeadm.go:309] [addons] Applied essential addon: CoreDNS
I0505 10:19:30.527050   52118 kubeadm.go:309] [addons] Applied essential addon: kube-proxy
I0505 10:19:30.528128   52118 kubeadm.go:309] 
I0505 10:19:30.528208   52118 kubeadm.go:309] Your Kubernetes control-plane has initialized successfully!
I0505 10:19:30.528215   52118 kubeadm.go:309] 
I0505 10:19:30.528336   52118 kubeadm.go:309] To start using your cluster, you need to run the following as a regular user:
I0505 10:19:30.528344   52118 kubeadm.go:309] 
I0505 10:19:30.528395   52118 kubeadm.go:309]   mkdir -p $HOME/.kube
I0505 10:19:30.528739   52118 kubeadm.go:309]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0505 10:19:30.528814   52118 kubeadm.go:309]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0505 10:19:30.528819   52118 kubeadm.go:309] 
I0505 10:19:30.528901   52118 kubeadm.go:309] Alternatively, if you are the root user, you can run:
I0505 10:19:30.528911   52118 kubeadm.go:309] 
I0505 10:19:30.528976   52118 kubeadm.go:309]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0505 10:19:30.528980   52118 kubeadm.go:309] 
I0505 10:19:30.529057   52118 kubeadm.go:309] You should now deploy a pod network to the cluster.
I0505 10:19:30.529177   52118 kubeadm.go:309] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0505 10:19:30.529286   52118 kubeadm.go:309]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0505 10:19:30.529291   52118 kubeadm.go:309] 
I0505 10:19:30.529415   52118 kubeadm.go:309] You can now join any number of control-plane nodes by copying certificate authorities
I0505 10:19:30.529520   52118 kubeadm.go:309] and service account keys on each node and then running the following as root:
I0505 10:19:30.529527   52118 kubeadm.go:309] 
I0505 10:19:30.529642   52118 kubeadm.go:309]   kubeadm join control-plane.minikube.internal:8443 --token pterr3.zljc46roqm4whle9 \
I0505 10:19:30.529793   52118 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:583745cd41efbc62bf8d6f77319cd59f6c658d9e5f8a8ab2fba5ac039a7cf2c1 \
I0505 10:19:30.529829   52118 kubeadm.go:309] 	--control-plane 
I0505 10:19:30.529833   52118 kubeadm.go:309] 
I0505 10:19:30.529947   52118 kubeadm.go:309] Then you can join any number of worker nodes by running the following on each as root:
I0505 10:19:30.529954   52118 kubeadm.go:309] 
I0505 10:19:30.530060   52118 kubeadm.go:309] kubeadm join control-plane.minikube.internal:8443 --token pterr3.zljc46roqm4whle9 \
I0505 10:19:30.530199   52118 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:583745cd41efbc62bf8d6f77319cd59f6c658d9e5f8a8ab2fba5ac039a7cf2c1 
I0505 10:19:30.544336   52118 kubeadm.go:309] 	[WARNING Swap]: swap is supported for cgroup v2 only; the NodeSwap feature gate of the kubelet is beta but disabled by default
I0505 10:19:30.544824   52118 kubeadm.go:309] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0505 10:19:30.545036   52118 cni.go:84] Creating CNI manager for ""
I0505 10:19:30.545062   52118 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0505 10:19:30.551422   52118 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0505 10:19:30.562286   52118 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0505 10:19:30.593612   52118 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0505 10:19:30.619421   52118 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0505 10:19:30.620091   52118 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0505 10:19:30.622502   52118 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_05_05T10_19_30_0700 minikube.k8s.io/version=v1.33.0 minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0505 10:19:30.631024   52118 ops.go:34] apiserver oom_adj: -16
I0505 10:19:30.996455   52118 kubeadm.go:1107] duration metric: took 376.987459ms to wait for elevateKubeSystemPrivileges
W0505 10:19:31.005878   52118 kubeadm.go:286] apiserver tunnel failed: apiserver port not set
I0505 10:19:31.007270   52118 kubeadm.go:393] duration metric: took 12.880448583s to StartCluster
I0505 10:19:31.007689   52118 settings.go:142] acquiring lock: {Name:mk1af8430d1e02178e4083cedf1f27a0c812b6fc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0505 10:19:31.013374   52118 settings.go:150] Updating kubeconfig:  /Users/gihan/.kube/config
I0505 10:19:31.057264   52118 lock.go:35] WriteFile acquiring /Users/gihan/.kube/config: {Name:mkb317f57513faa6dc017a2d7d26b4a9285c6434 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0505 10:19:31.057554   52118 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0505 10:19:31.067909   52118 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0505 10:19:31.080338   52118 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0505 10:19:31.076442   52118 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0505 10:19:31.082139   52118 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0505 10:19:31.082150   52118 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0505 10:19:31.082234   52118 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0505 10:19:31.149409   52118 out.go:177] 🔎  Verifying Kubernetes components...
I0505 10:19:31.084412   52118 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0505 10:19:31.102946   52118 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0505 10:19:31.130044   52118 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0505 10:19:31.149771   52118 host.go:66] Checking if "minikube" exists ...
I0505 10:19:31.157346   52118 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0505 10:19:31.157687   52118 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0505 10:19:31.279869   52118 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0505 10:19:31.339191   52118 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0505 10:19:31.345184   52118 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0505 10:19:31.345189   52118 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0505 10:19:31.345245   52118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0505 10:19:31.348474   52118 addons.go:234] Setting addon default-storageclass=true in "minikube"
I0505 10:19:31.348503   52118 host.go:66] Checking if "minikube" exists ...
I0505 10:19:31.348785   52118 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0505 10:19:31.422980   52118 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58254 SSHKeyPath:/Users/gihan/.minikube/machines/minikube/id_rsa Username:docker}
I0505 10:19:31.423190   52118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0505 10:19:31.424604   52118 start.go:946] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0505 10:19:31.422848   52118 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0505 10:19:31.424736   52118 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0505 10:19:31.429956   52118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0505 10:19:31.478361   52118 api_server.go:52] waiting for apiserver process to appear ...
I0505 10:19:31.478453   52118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0505 10:19:31.491717   52118 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58254 SSHKeyPath:/Users/gihan/.minikube/machines/minikube/id_rsa Username:docker}
I0505 10:19:31.502017   52118 api_server.go:72] duration metric: took 421.594083ms to wait for apiserver process to appear ...
I0505 10:19:31.502029   52118 api_server.go:88] waiting for apiserver healthz status ...
I0505 10:19:31.502041   52118 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58253/healthz ...
I0505 10:19:31.522847   52118 api_server.go:279] https://127.0.0.1:58253/healthz returned 200:
ok
I0505 10:19:31.523686   52118 api_server.go:141] control plane version: v1.30.0
I0505 10:19:31.523692   52118 api_server.go:131] duration metric: took 21.660083ms to wait for apiserver health ...
I0505 10:19:31.523849   52118 system_pods.go:43] waiting for kube-system pods to appear ...
I0505 10:19:31.551750   52118 system_pods.go:59] 4 kube-system pods found
I0505 10:19:31.551767   52118 system_pods.go:61] "etcd-minikube" [4d66f292-17b7-4b0d-8373-3da613032fd3] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0505 10:19:31.551772   52118 system_pods.go:61] "kube-apiserver-minikube" [51e1d35b-ce57-4d38-a510-0f17956fea25] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0505 10:19:31.551778   52118 system_pods.go:61] "kube-controller-manager-minikube" [54e669e3-d7be-4718-82a3-fe97fa4d8307] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0505 10:19:31.551781   52118 system_pods.go:61] "kube-scheduler-minikube" [bb89fb6a-d707-423a-a47f-ae6ddeb571a5] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0505 10:19:31.551784   52118 system_pods.go:74] duration metric: took 27.9325ms to wait for pod list to return data ...
I0505 10:19:31.551789   52118 kubeadm.go:576] duration metric: took 471.370958ms to wait for: map[apiserver:true system_pods:true]
I0505 10:19:31.551794   52118 node_conditions.go:102] verifying NodePressure condition ...
I0505 10:19:31.554060   52118 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0505 10:19:31.554080   52118 node_conditions.go:123] node cpu capacity is 8
I0505 10:19:31.554096   52118 node_conditions.go:105] duration metric: took 2.299833ms to run NodePressure ...
I0505 10:19:31.554102   52118 start.go:240] waiting for startup goroutines ...
I0505 10:19:31.631244   52118 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0505 10:19:31.639079   52118 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0505 10:19:31.849215   52118 out.go:177] 🌟  Enabled addons: default-storageclass, storage-provisioner
I0505 10:19:31.858042   52118 addons.go:505] duration metric: took 783.217792ms for enable addons: enabled=[default-storageclass storage-provisioner]
I0505 10:19:31.934487   52118 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0505 10:19:31.934512   52118 start.go:245] waiting for cluster config update ...
I0505 10:19:31.934523   52118 start.go:254] writing updated cluster config ...
I0505 10:19:31.936173   52118 ssh_runner.go:195] Run: rm -f paused
I0505 10:19:32.711664   52118 start.go:600] kubectl: 1.30.0, cluster: 1.30.0 (minor skew: 0)
I0505 10:19:32.716013   52118 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 05 08:49:01 minikube cri-dockerd[1329]: time="2024-05-05T08:49:01Z" level=info msg="Pulling image gihaa/ds-project-payment:latest: 22dae6400ad3: Downloading [=============================>                     ]   33.9MB/56.77MB"
May 05 08:49:11 minikube cri-dockerd[1329]: time="2024-05-05T08:49:11Z" level=info msg="Pulling image gihaa/ds-project-payment:latest: 22dae6400ad3: Downloading [==================================>                ]  39.29MB/56.77MB"
May 05 08:49:21 minikube cri-dockerd[1329]: time="2024-05-05T08:49:21Z" level=info msg="Pulling image gihaa/ds-project-payment:latest: 22dae6400ad3: Downloading [=====================================>             ]  43.07MB/56.77MB"
May 05 08:49:31 minikube cri-dockerd[1329]: time="2024-05-05T08:49:31Z" level=info msg="Pulling image gihaa/ds-project-payment:latest: 22dae6400ad3: Downloading [==========================================>        ]  47.92MB/56.77MB"
May 05 08:49:41 minikube cri-dockerd[1329]: time="2024-05-05T08:49:41Z" level=info msg="Pulling image gihaa/ds-project-payment:latest: 22dae6400ad3: Downloading [==============================================>    ]  52.76MB/56.77MB"
May 05 08:49:51 minikube cri-dockerd[1329]: time="2024-05-05T08:49:51Z" level=info msg="Pulling image gihaa/ds-project-payment:latest: 22dae6400ad3: Extracting [==========>                                        ]  12.26MB/56.77MB"
May 05 08:50:01 minikube cri-dockerd[1329]: time="2024-05-05T08:50:01Z" level=info msg="Pulling image gihaa/ds-project-payment:latest: 22dae6400ad3: Extracting [================================================>  ]  54.59MB/56.77MB"
May 05 08:50:03 minikube cri-dockerd[1329]: time="2024-05-05T08:50:03Z" level=info msg="Stop pulling image gihaa/ds-project-payment:latest: Status: Downloaded newer image for gihaa/ds-project-payment:latest"
May 05 08:50:04 minikube dockerd[1099]: time="2024-05-05T08:50:04.500348053Z" level=warning msg="reference for unknown type: " digest="sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366" remote="registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366" spanID=efc98c013b22e156 traceID=f5b526f09de4b345b35ddd943acdb118
May 05 08:50:15 minikube cri-dockerd[1329]: time="2024-05-05T08:50:15Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366: 780d03f224b3: Downloading [=>                                                 ]  436.6kB/21.3MB"
May 05 08:50:25 minikube cri-dockerd[1329]: time="2024-05-05T08:50:25Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366: 780d03f224b3: Downloading [=============>                                     ]   5.84MB/21.3MB"
May 05 08:50:35 minikube cri-dockerd[1329]: time="2024-05-05T08:50:35Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366: 780d03f224b3: Downloading [===============================>                   ]  13.39MB/21.3MB"
May 05 08:50:45 minikube cri-dockerd[1329]: time="2024-05-05T08:50:45Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366"
May 05 08:50:45 minikube cri-dockerd[1329]: time="2024-05-05T08:50:45Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366"
May 05 08:50:45 minikube dockerd[1099]: time="2024-05-05T08:50:45.943188961Z" level=info msg="ignoring event" container=8459fde67a8d0615f9cf324ba17c809cb774e4d4ddb8492d7b67f91166b92cdc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 08:50:45 minikube dockerd[1099]: time="2024-05-05T08:50:45.958134127Z" level=info msg="ignoring event" container=8f20c421061da72efefb427738adba9b6d03b9173a0175544360e63555c52483 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 08:50:46 minikube dockerd[1099]: time="2024-05-05T08:50:46.556421961Z" level=info msg="ignoring event" container=98fc446c9b790b8ae5856ed7664366b37e11a49c94ea2038ac7e64e4c026307a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 08:50:47 minikube dockerd[1099]: time="2024-05-05T08:50:47.376737753Z" level=info msg="ignoring event" container=67b4913659930a772f6590fac1469db45d67f677ad7fdb0e91aa56bfae3127c6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 08:50:48 minikube dockerd[1099]: time="2024-05-05T08:50:48.407834837Z" level=info msg="ignoring event" container=9d17caab0b2a7f15a54e9836417e68629731d5bc7f2a456ae5dddbd0d6b3182e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 08:52:02 minikube cri-dockerd[1329]: time="2024-05-05T08:52:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8b5882aff7986282504fe3b762b0455c381707bad64f992dc8739c79d3dd330b/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 05 08:52:03 minikube dockerd[1099]: time="2024-05-05T08:52:03.088442594Z" level=warning msg="reference for unknown type: " digest="sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e" remote="registry.k8s.io/ingress-nginx/controller@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e" spanID=336f4952dc304193 traceID=f6617778b353ef213b45efe50694408f
May 05 08:52:14 minikube cri-dockerd[1329]: time="2024-05-05T08:52:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.1@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e: bca4290a9639: Downloading [=============>                                     ]  872.9kB/3.348MB"
May 05 08:52:24 minikube cri-dockerd[1329]: time="2024-05-05T08:52:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.1@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e: 1ae4c16f76ce: Downloading [=================>                                 ]  1.691MB/4.928MB"
May 05 08:52:34 minikube cri-dockerd[1329]: time="2024-05-05T08:52:34Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.1@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e: 1ae4c16f76ce: Downloading [=====================================>             ]   3.65MB/4.928MB"
May 05 08:52:44 minikube cri-dockerd[1329]: time="2024-05-05T08:52:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.1@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e: 522deef7356a: Downloading [===================>                               ]  12.76MB/33.42MB"
May 05 08:52:54 minikube cri-dockerd[1329]: time="2024-05-05T08:52:54Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.1@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e: 522deef7356a: Downloading [==============================>                    ]  20.68MB/33.42MB"
May 05 08:53:04 minikube cri-dockerd[1329]: time="2024-05-05T08:53:04Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.1@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e: 522deef7356a: Downloading [==========================================>        ]  28.25MB/33.42MB"
May 05 08:53:14 minikube cri-dockerd[1329]: time="2024-05-05T08:53:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.1@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e: e4c3c226a9cf: Extracting [===============================================>   ]  753.7kB/787.1kB"
May 05 08:53:24 minikube cri-dockerd[1329]: time="2024-05-05T08:53:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.1@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e: e269a84df58a: Downloading [=========================>                         ]  10.75MB/20.97MB"
May 05 08:53:34 minikube cri-dockerd[1329]: time="2024-05-05T08:53:34Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.1@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e: e269a84df58a: Downloading [========================================>          ]  17.14MB/20.97MB"
May 05 08:53:44 minikube cri-dockerd[1329]: time="2024-05-05T08:53:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.1@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e: 3a5f5540f4dd: Downloading [===============>                                   ]    905kB/2.857MB"
May 05 08:53:54 minikube cri-dockerd[1329]: time="2024-05-05T08:53:54Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.1@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e: 70e3fb491a4d: Downloading [================>                                  ]  3.943MB/12.11MB"
May 05 08:54:04 minikube cri-dockerd[1329]: time="2024-05-05T08:54:04Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.1@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e: 70e3fb491a4d: Downloading [============================================>      ]  10.73MB/12.11MB"
May 05 08:54:14 minikube cri-dockerd[1329]: time="2024-05-05T08:54:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.1@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e: 3c093e41c8b6: Downloading [=================>                                 ]  6.087MB/17.39MB"
May 05 08:54:24 minikube cri-dockerd[1329]: time="2024-05-05T08:54:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.10.1@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e: 3c093e41c8b6: Downloading [==========================================>        ]  14.74MB/17.39MB"
May 05 08:54:27 minikube cri-dockerd[1329]: time="2024-05-05T08:54:27Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/controller:v1.10.1@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/controller@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e"
May 05 08:55:26 minikube cri-dockerd[1329]: time="2024-05-05T08:55:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/83d5d0666ca43884fcfa8639ad5842bc060e9867b2d1e051464c09d43cd394c3/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 05 08:55:26 minikube cri-dockerd[1329]: time="2024-05-05T08:55:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1758c3cb5e36064990828a69cd6e0f396db2491ba376465530e9de34045e7ee4/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 05 08:55:26 minikube dockerd[1099]: time="2024-05-05T08:55:26.655055132Z" level=warning msg="reference for unknown type: " digest="sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334" remote="registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334" spanID=c159563165b5f564 traceID=a7229747d5847919a43b8fd69adba51f
May 05 08:55:34 minikube dockerd[1099]: time="2024-05-05T08:55:34.353983886Z" level=info msg="ignoring event" container=3f7ee71e3ccfe7a15d954c9208b968889cdebbb67c55614de8609af834bd81fe module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 08:55:34 minikube dockerd[1099]: time="2024-05-05T08:55:34.568154886Z" level=info msg="ignoring event" container=8b5882aff7986282504fe3b762b0455c381707bad64f992dc8739c79d3dd330b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 08:55:37 minikube cri-dockerd[1329]: time="2024-05-05T08:55:37Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: a8b58160bdad: Downloading [===>                                               ]  1.306MB/20.98MB"
May 05 08:55:47 minikube cri-dockerd[1329]: time="2024-05-05T08:55:47Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: a8b58160bdad: Downloading [===============>                                   ]   6.44MB/20.98MB"
May 05 08:55:57 minikube cri-dockerd[1329]: time="2024-05-05T08:55:57Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: a8b58160bdad: Downloading [================================>                  ]  13.68MB/20.98MB"
May 05 08:56:07 minikube cri-dockerd[1329]: time="2024-05-05T08:56:07Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: a8b58160bdad: Downloading [================================================>  ]   20.5MB/20.98MB"
May 05 08:56:08 minikube cri-dockerd[1329]: time="2024-05-05T08:56:08Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334"
May 05 08:56:09 minikube dockerd[1099]: time="2024-05-05T08:56:09.084169930Z" level=info msg="ignoring event" container=15488b7025dd3061600b387481c2dc3be1362cc1cc98df8f05c2f6b54cd2be3c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 08:56:09 minikube cri-dockerd[1329]: time="2024-05-05T08:56:09Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334"
May 05 08:56:09 minikube dockerd[1099]: time="2024-05-05T08:56:09.316960888Z" level=info msg="ignoring event" container=6a7f45ee259dc0e1652e072a93cacf15f614fa23e06156f6d8f3a5be1f4b6b6d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 08:56:10 minikube dockerd[1099]: time="2024-05-05T08:56:10.795923291Z" level=info msg="ignoring event" container=83d5d0666ca43884fcfa8639ad5842bc060e9867b2d1e051464c09d43cd394c3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 08:56:10 minikube dockerd[1099]: time="2024-05-05T08:56:10.796098583Z" level=info msg="ignoring event" container=1758c3cb5e36064990828a69cd6e0f396db2491ba376465530e9de34045e7ee4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 08:58:27 minikube cri-dockerd[1329]: E0505 08:58:27.884871    1329 attach.go:53] error attaching to container: EOF
May 05 08:58:28 minikube cri-dockerd[1329]: time="2024-05-05T08:58:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3404a2975a232f8a35f1a67f93978898eda7b1b4c898d97e3a30f6750040a9ce/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 05 08:58:28 minikube cri-dockerd[1329]: time="2024-05-05T08:58:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e90552a00ab60abb741c849c449a42ea276250fc4f328cd3edb83b868f02f148/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 05 08:58:29 minikube dockerd[1099]: time="2024-05-05T08:58:29.478164342Z" level=info msg="ignoring event" container=2e6cc4eae58dfa4408b72fe93efcf7c41a54aa958a6da80b9cf06f15128ba57a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 08:58:29 minikube dockerd[1099]: time="2024-05-05T08:58:29.489249717Z" level=info msg="ignoring event" container=3a8eddded647ae32e33a5e195af824e75495a5424cec012301cde68551130688 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 08:58:30 minikube dockerd[1099]: time="2024-05-05T08:58:30.413458967Z" level=info msg="ignoring event" container=2920a476751d95637420299c88917b24b273251a438e93530e7061ac54e52f09 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 08:58:31 minikube dockerd[1099]: time="2024-05-05T08:58:31.313571134Z" level=info msg="ignoring event" container=e90552a00ab60abb741c849c449a42ea276250fc4f328cd3edb83b868f02f148 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 08:58:32 minikube dockerd[1099]: time="2024-05-05T08:58:32.410520010Z" level=info msg="ignoring event" container=3404a2975a232f8a35f1a67f93978898eda7b1b4c898d97e3a30f6750040a9ce module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 09:05:52 minikube cri-dockerd[1329]: E0505 09:05:52.400892    1329 attach.go:53] error attaching to container: EOF


==> container status <==
CONTAINER           IMAGE                                                                                                  CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
2920a476751d9       1a024e390dd05                                                                                          25 minutes ago      Exited              patch                     1                   3404a2975a232       ingress-nginx-admission-patch-6vbwp
3a8eddded647a       1a024e390dd05                                                                                          25 minutes ago      Exited              create                    0                   e90552a00ab60       ingress-nginx-admission-create-hq5qj
56869c16d455e       gihaa/ds-project-payment@sha256:729737642902e0033f3fe7779a93695a7c68509a362b236d7dbbb62da7177086       33 minutes ago      Running             payment-service           0                   cdbe8cb2fa8e0       payment-5b55fd9b94-dbj84
2fc8074e3fe85       gihaa/ds-project-enrollment@sha256:8d686e76b084da50f0b9be058a0d2d8ae0f201f08745eca35282ed36d68a5b22    35 minutes ago      Running             enrollment-service        0                   e4fe0739061df       enrollment-7fd9c9bd98-n58j2
8bb2391c1d019       gihaa/ds-project-course@sha256:e626ec1830eb7575463a45b622ca065adbf7ae0847aca6d0799dc475bd881bbe        38 minutes ago      Running             course-service            0                   ae8d05404a4f3       course-9f595bf5-4469v
7472613d21e6a       gihaa/ds-project-auth@sha256:bff832c0d40f2ac24c2d559ca45d210ae3e2c43052a8a00e3931523ad3b1664d          41 minutes ago      Running             auth-service              0                   f27f0045d3f84       auth-bffc9797b-67xqg
8612239c261b3       gihaa/ds-project-nginx-proxy@sha256:60fb98b062b08da0b789b4a67eda098d58f39d0c4df91e8ec7bb1b75c4fef04d   44 minutes ago      Running             nginx-proxy               0                   dd55aba604dac       nginx-proxy-f555d7f4d-66twd
198e788276ad4       ba04bb24b9575                                                                                          45 minutes ago      Running             storage-provisioner       4                   871714592ba8c       storage-provisioner
b36a1c9081d52       ba04bb24b9575                                                                                          4 hours ago         Exited              storage-provisioner       3                   871714592ba8c       storage-provisioner
5859f4c835ce5       2437cf7621777                                                                                          5 hours ago         Running             coredns                   0                   1a2a7c1933434       coredns-7db6d8ff4d-fjjk9
c3ceae517074e       cb7eac0b42cc1                                                                                          5 hours ago         Running             kube-proxy                0                   b44bb746e89a6       kube-proxy-2ph9x
77c375ed2c5a9       181f57fd3cdb7                                                                                          5 hours ago         Running             kube-apiserver            0                   83903ba09bfd2       kube-apiserver-minikube
e45ed06e16f68       68feac521c0f1                                                                                          5 hours ago         Running             kube-controller-manager   0                   6a81489268ef8       kube-controller-manager-minikube
c704a7a79e738       014faa467e297                                                                                          5 hours ago         Running             etcd                      0                   7a33f1043a5d1       etcd-minikube
0b63c8528a8fd       547adae34140b                                                                                          5 hours ago         Running             kube-scheduler            0                   89d478b59e8a1       kube-scheduler-minikube


==> coredns [5859f4c835ce] <==
[INFO] 10.244.0.47:39887 - 57345 "A IN ac-oxe8thw-shard-00-00.etrzu5q.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000113292s
[INFO] 10.244.0.47:39887 - 57678 "AAAA IN ac-oxe8thw-shard-00-00.etrzu5q.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.00013825s
[INFO] 10.244.0.47:59661 - 3912 "A IN ac-oxe8thw-shard-00-00.etrzu5q.mongodb.net. udp 60 false 512" NOERROR qr,rd,ra 344 0.004839875s
[INFO] 10.244.0.47:59661 - 4203 "AAAA IN ac-oxe8thw-shard-00-00.etrzu5q.mongodb.net. udp 60 false 512" NOERROR qr,rd,ra 278 0.079360833s
[INFO] 10.244.0.49:32799 - 47755 "AAAA IN ac-oxe8thw-shard-00-00.etrzu5q.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.00036675s
[INFO] 10.244.0.49:32799 - 47255 "A IN ac-oxe8thw-shard-00-00.etrzu5q.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000528208s
[INFO] 10.244.0.49:55496 - 59992 "AAAA IN ac-oxe8thw-shard-00-00.etrzu5q.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000276208s
[INFO] 10.244.0.49:55496 - 59659 "A IN ac-oxe8thw-shard-00-00.etrzu5q.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.00034475s
[INFO] 10.244.0.49:58682 - 30655 "AAAA IN ac-oxe8thw-shard-00-00.etrzu5q.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000171417s
[INFO] 10.244.0.49:58682 - 30280 "A IN ac-oxe8thw-shard-00-00.etrzu5q.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000278s
[INFO] 10.244.0.49:39870 - 2277 "AAAA IN ac-oxe8thw-shard-00-00.etrzu5q.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 278 0.000135083s
[INFO] 10.244.0.49:39870 - 1944 "A IN ac-oxe8thw-shard-00-00.etrzu5q.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 344 0.000200416s
[INFO] 10.244.0.49:59869 - 13820 "AAAA IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000196625s
[INFO] 10.244.0.49:59869 - 12445 "A IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000211042s
[INFO] 10.244.0.49:47735 - 25714 "AAAA IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.00010725s
[INFO] 10.244.0.49:47735 - 25047 "A IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.0002335s
[INFO] 10.244.0.49:40144 - 41947 "AAAA IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000078292s
[INFO] 10.244.0.49:40144 - 41030 "A IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000151625s
[INFO] 10.244.0.49:37191 - 12250 "A IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 344 0.000224375s
[INFO] 10.244.0.49:37191 - 13000 "AAAA IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net. udp 60 false 512" NOERROR qr,rd,ra 60 0.017722875s
[INFO] 10.244.0.48:44972 - 54274 "AAAA IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000146416s
[INFO] 10.244.0.48:44972 - 53649 "A IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000046167s
[INFO] 10.244.0.48:35899 - 7025 "AAAA IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000043375s
[INFO] 10.244.0.48:35899 - 6858 "A IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000077875s
[INFO] 10.244.0.48:36471 - 63767 "AAAA IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000043917s
[INFO] 10.244.0.48:36471 - 63642 "A IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000064166s
[INFO] 10.244.0.48:58915 - 3793 "AAAA IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 60 0.000041875s
[INFO] 10.244.0.48:58915 - 3376 "A IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 344 0.000047375s
[INFO] 10.244.0.49:46490 - 38644 "AAAA IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.001481541s
[INFO] 10.244.0.49:55438 - 4970 "AAAA IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000160834s
[INFO] 10.244.0.49:46490 - 38311 "A IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.001460417s
[INFO] 10.244.0.49:55438 - 4845 "A IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000122875s
[INFO] 10.244.0.49:49009 - 55799 "AAAA IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000123084s
[INFO] 10.244.0.49:49009 - 55334 "A IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.0000295s
[INFO] 10.244.0.49:38459 - 21094 "AAAA IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 277 0.000143833s
[INFO] 10.244.0.49:38459 - 20678 "A IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 342 0.000120916s
[INFO] 10.244.0.47:41286 - 13381 "AAAA IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000552292s
[INFO] 10.244.0.47:41286 - 12964 "A IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000248417s
[INFO] 10.244.0.47:48890 - 21726 "AAAA IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000060125s
[INFO] 10.244.0.47:48890 - 21393 "A IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.00002075s
[INFO] 10.244.0.47:37084 - 21168 "AAAA IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000021417s
[INFO] 10.244.0.47:37084 - 20959 "A IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000017833s
[INFO] 10.244.0.47:50273 - 21431 "AAAA IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 277 0.000043333s
[INFO] 10.244.0.47:50273 - 21181 "A IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 342 0.000029833s
[INFO] 10.244.0.48:45761 - 51569 "AAAA IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.0000675s
[INFO] 10.244.0.48:45761 - 51319 "A IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000559792s
[INFO] 10.244.0.48:49364 - 31233 "AAAA IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000026917s
[INFO] 10.244.0.48:49364 - 31024 "A IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000033459s
[INFO] 10.244.0.48:43254 - 45413 "AAAA IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000032667s
[INFO] 10.244.0.48:43254 - 45288 "A IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000048625s
[INFO] 10.244.0.48:55229 - 61509 "AAAA IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 277 0.000026833s
[INFO] 10.244.0.48:55229 - 61259 "A IN ac-oxe8thw-shard-00-01.etrzu5q.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 342 0.000023833s
[INFO] 10.244.0.46:45172 - 33023 "AAAA IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000240625s
[INFO] 10.244.0.46:45172 - 32565 "A IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.0003515s
[INFO] 10.244.0.46:34474 - 47123 "AAAA IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000041s
[INFO] 10.244.0.46:34474 - 46790 "A IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000078375s
[INFO] 10.244.0.46:49250 - 60656 "AAAA IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000104958s
[INFO] 10.244.0.46:49250 - 60191 "A IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000090291s
[INFO] 10.244.0.46:57983 - 45648 "AAAA IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 60 0.000053084s
[INFO] 10.244.0.46:57983 - 45398 "A IN ac-oxe8thw-shard-00-02.etrzu5q.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 344 0.000062708s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_05_05T10_19_30_0700
                    minikube.k8s.io/version=v1.33.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 05 May 2024 04:49:27 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 05 May 2024 09:23:43 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 05 May 2024 09:21:46 +0000   Sun, 05 May 2024 05:13:52 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 05 May 2024 09:21:46 +0000   Sun, 05 May 2024 05:13:52 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 05 May 2024 09:21:46 +0000   Sun, 05 May 2024 05:13:52 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 05 May 2024 09:21:46 +0000   Sun, 05 May 2024 05:13:52 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             4017408Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             4017408Ki
  pods:               110
System Info:
  Machine ID:                 1ed294ce072441858e7206067d5e1762
  System UUID:                1ed294ce072441858e7206067d5e1762
  Boot ID:                    656d8ea3-7a6b-463b-b6d0-da9b13a1aea6
  Kernel Version:             6.6.12-linuxkit
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://26.0.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     auth-bffc9797b-67xqg                0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         44m
  default                     course-9f595bf5-4469v               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         44m
  default                     enrollment-7fd9c9bd98-n58j2         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         44m
  default                     nginx-proxy-f555d7f4d-66twd         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         44m
  default                     payment-5b55fd9b94-dbj84            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         44m
  kube-system                 coredns-7db6d8ff4d-fjjk9            100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     4h34m
  kube-system                 etcd-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         4h34m
  kube-system                 kube-apiserver-minikube             250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h34m
  kube-system                 kube-controller-manager-minikube    200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h34m
  kube-system                 kube-proxy-2ph9x                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h34m
  kube-system                 kube-scheduler-minikube             100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h34m
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h34m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (4%!)(MISSING)  170Mi (4%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-32Mi     0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-64Ki     0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>


==> dmesg <==
[May 5 05:50] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.211489] netlink: 'init': attribute type 4 has an invalid length.
[  +0.010440] fakeowner: loading out-of-tree module taints kernel.
[ +22.674029] systemd[1393]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set


==> etcd [c704a7a79e73] <==
{"level":"info","ts":"2024-05-05T08:41:10.901713Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1875501644,"revision":10348,"compact-revision":10109}
{"level":"info","ts":"2024-05-05T08:42:14.374797Z","caller":"traceutil/trace.go:171","msg":"trace[692187583] transaction","detail":"{read_only:false; response_revision:10807; number_of_response:1; }","duration":"115.161792ms","start":"2024-05-05T08:42:14.258435Z","end":"2024-05-05T08:42:14.373596Z","steps":["trace[692187583] 'process raft request'  (duration: 114.645125ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T08:45:07.220567Z","caller":"traceutil/trace.go:171","msg":"trace[658586319] transaction","detail":"{read_only:false; response_revision:10952; number_of_response:1; }","duration":"122.68075ms","start":"2024-05-05T08:45:07.092497Z","end":"2024-05-05T08:45:07.215178Z","steps":["trace[658586319] 'process raft request'  (duration: 122.036042ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T08:45:11.443057Z","caller":"traceutil/trace.go:171","msg":"trace[713120790] linearizableReadLoop","detail":"{readStateIndex:13163; appliedIndex:13162; }","duration":"188.273583ms","start":"2024-05-05T08:45:11.254695Z","end":"2024-05-05T08:45:11.442969Z","steps":["trace[713120790] 'read index received'  (duration: 187.353833ms)","trace[713120790] 'applied index is now lower than readState.Index'  (duration: 919.375µs)"],"step_count":2}
{"level":"info","ts":"2024-05-05T08:45:11.443447Z","caller":"traceutil/trace.go:171","msg":"trace[801551333] transaction","detail":"{read_only:false; response_revision:10955; number_of_response:1; }","duration":"200.885125ms","start":"2024-05-05T08:45:11.242497Z","end":"2024-05-05T08:45:11.443382Z","steps":["trace[801551333] 'process raft request'  (duration: 199.728708ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-05T08:45:11.444662Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"189.23ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:601"}
{"level":"info","ts":"2024-05-05T08:45:11.444905Z","caller":"traceutil/trace.go:171","msg":"trace[922828469] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:10955; }","duration":"190.179875ms","start":"2024-05-05T08:45:11.25467Z","end":"2024-05-05T08:45:11.44485Z","steps":["trace[922828469] 'agreement among raft nodes before linearized reading'  (duration: 188.701916ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T08:45:15.69741Z","caller":"traceutil/trace.go:171","msg":"trace[770089425] linearizableReadLoop","detail":"{readStateIndex:13168; appliedIndex:13167; }","duration":"165.033542ms","start":"2024-05-05T08:45:15.532276Z","end":"2024-05-05T08:45:15.697309Z","steps":["trace[770089425] 'read index received'  (duration: 164.57075ms)","trace[770089425] 'applied index is now lower than readState.Index'  (duration: 462.459µs)"],"step_count":2}
{"level":"warn","ts":"2024-05-05T08:45:15.69833Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"165.8675ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/\" range_end:\"/registry/services/specs0\" ","response":"range_response_count:7 size:8441"}
{"level":"info","ts":"2024-05-05T08:45:15.698396Z","caller":"traceutil/trace.go:171","msg":"trace[605417714] range","detail":"{range_begin:/registry/services/specs/; range_end:/registry/services/specs0; response_count:7; response_revision:10959; }","duration":"166.102333ms","start":"2024-05-05T08:45:15.532273Z","end":"2024-05-05T08:45:15.698376Z","steps":["trace[605417714] 'agreement among raft nodes before linearized reading'  (duration: 165.412875ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T08:45:15.698352Z","caller":"traceutil/trace.go:171","msg":"trace[1609920010] transaction","detail":"{read_only:false; response_revision:10959; number_of_response:1; }","duration":"192.595333ms","start":"2024-05-05T08:45:15.505728Z","end":"2024-05-05T08:45:15.698323Z","steps":["trace[1609920010] 'process raft request'  (duration: 191.231167ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T08:45:17.969389Z","caller":"traceutil/trace.go:171","msg":"trace[2051422914] transaction","detail":"{read_only:false; response_revision:10960; number_of_response:1; }","duration":"236.922375ms","start":"2024-05-05T08:45:17.73241Z","end":"2024-05-05T08:45:17.969332Z","steps":["trace[2051422914] 'process raft request'  (duration: 236.203333ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T08:45:17.969352Z","caller":"traceutil/trace.go:171","msg":"trace[249083447] linearizableReadLoop","detail":"{readStateIndex:13169; appliedIndex:13168; }","duration":"230.178667ms","start":"2024-05-05T08:45:17.739048Z","end":"2024-05-05T08:45:17.969227Z","steps":["trace[249083447] 'read index received'  (duration: 229.321292ms)","trace[249083447] 'applied index is now lower than readState.Index'  (duration: 856.959µs)"],"step_count":2}
{"level":"warn","ts":"2024-05-05T08:45:17.970266Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"230.864792ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/\" range_end:\"/registry/services/specs0\" ","response":"range_response_count:7 size:8441"}
{"level":"info","ts":"2024-05-05T08:45:17.970448Z","caller":"traceutil/trace.go:171","msg":"trace[924804343] range","detail":"{range_begin:/registry/services/specs/; range_end:/registry/services/specs0; response_count:7; response_revision:10960; }","duration":"231.339501ms","start":"2024-05-05T08:45:17.739042Z","end":"2024-05-05T08:45:17.970382Z","steps":["trace[924804343] 'agreement among raft nodes before linearized reading'  (duration: 230.577167ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T08:45:24.385084Z","caller":"traceutil/trace.go:171","msg":"trace[1130657950] linearizableReadLoop","detail":"{readStateIndex:13177; appliedIndex:13176; }","duration":"215.791458ms","start":"2024-05-05T08:45:24.169221Z","end":"2024-05-05T08:45:24.385013Z","steps":["trace[1130657950] 'read index received'  (duration: 215.585291ms)","trace[1130657950] 'applied index is now lower than readState.Index'  (duration: 205.709µs)"],"step_count":2}
{"level":"info","ts":"2024-05-05T08:45:24.385393Z","caller":"traceutil/trace.go:171","msg":"trace[1479018235] transaction","detail":"{read_only:false; response_revision:10966; number_of_response:1; }","duration":"369.234625ms","start":"2024-05-05T08:45:24.016135Z","end":"2024-05-05T08:45:24.38537Z","steps":["trace[1479018235] 'process raft request'  (duration: 368.6975ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-05T08:45:24.385726Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"216.411333ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/\" range_end:\"/registry/services/specs0\" ","response":"range_response_count:7 size:8441"}
{"level":"info","ts":"2024-05-05T08:45:24.385854Z","caller":"traceutil/trace.go:171","msg":"trace[446733472] range","detail":"{range_begin:/registry/services/specs/; range_end:/registry/services/specs0; response_count:7; response_revision:10966; }","duration":"216.598292ms","start":"2024-05-05T08:45:24.169218Z","end":"2024-05-05T08:45:24.385816Z","steps":["trace[446733472] 'agreement among raft nodes before linearized reading'  (duration: 216.173209ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-05T08:45:24.386682Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-05T08:45:24.016121Z","time spent":"369.344333ms","remote":"127.0.0.1:39758","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:10964 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-05-05T08:46:05.980389Z","caller":"traceutil/trace.go:171","msg":"trace[426572313] transaction","detail":"{read_only:false; response_revision:11070; number_of_response:1; }","duration":"180.007666ms","start":"2024-05-05T08:46:05.800303Z","end":"2024-05-05T08:46:05.980311Z","steps":["trace[426572313] 'process raft request'  (duration: 179.709541ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T08:46:10.849976Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10756}
{"level":"info","ts":"2024-05-05T08:46:10.898997Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":10756,"took":"45.969542ms","hash":445276736,"current-db-size-bytes":6955008,"current-db-size":"7.0 MB","current-db-size-in-use-bytes":2924544,"current-db-size-in-use":"2.9 MB"}
{"level":"info","ts":"2024-05-05T08:46:10.899267Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":445276736,"revision":10756,"compact-revision":10348}
{"level":"info","ts":"2024-05-05T08:47:40.581554Z","caller":"traceutil/trace.go:171","msg":"trace[1789595359] transaction","detail":"{read_only:false; response_revision:11149; number_of_response:1; }","duration":"106.315625ms","start":"2024-05-05T08:47:40.474602Z","end":"2024-05-05T08:47:40.580918Z","steps":["trace[1789595359] 'process raft request'  (duration: 103.318291ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T08:47:40.580331Z","caller":"traceutil/trace.go:171","msg":"trace[1121113064] linearizableReadLoop","detail":"{readStateIndex:13390; appliedIndex:13389; }","duration":"103.262584ms","start":"2024-05-05T08:47:40.476826Z","end":"2024-05-05T08:47:40.580088Z","steps":["trace[1121113064] 'read index received'  (duration: 100.789917ms)","trace[1121113064] 'applied index is now lower than readState.Index'  (duration: 2.471792ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-05T08:47:40.586738Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"108.288666ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-05-05T08:47:40.591762Z","caller":"traceutil/trace.go:171","msg":"trace[762576454] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:11149; }","duration":"114.919084ms","start":"2024-05-05T08:47:40.476803Z","end":"2024-05-05T08:47:40.591722Z","steps":["trace[762576454] 'agreement among raft nodes before linearized reading'  (duration: 105.092375ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T08:47:42.288427Z","caller":"traceutil/trace.go:171","msg":"trace[313305412] transaction","detail":"{read_only:false; response_revision:11150; number_of_response:1; }","duration":"162.826334ms","start":"2024-05-05T08:47:42.124629Z","end":"2024-05-05T08:47:42.287455Z","steps":["trace[313305412] 'process raft request'  (duration: 159.860375ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T08:47:46.865368Z","caller":"traceutil/trace.go:171","msg":"trace[1419138567] linearizableReadLoop","detail":"{readStateIndex:13397; appliedIndex:13396; }","duration":"120.212292ms","start":"2024-05-05T08:47:46.745066Z","end":"2024-05-05T08:47:46.865278Z","steps":["trace[1419138567] 'read index received'  (duration: 119.721459ms)","trace[1419138567] 'applied index is now lower than readState.Index'  (duration: 490.375µs)"],"step_count":2}
{"level":"info","ts":"2024-05-05T08:47:46.865869Z","caller":"traceutil/trace.go:171","msg":"trace[530736290] transaction","detail":"{read_only:false; response_revision:11155; number_of_response:1; }","duration":"141.377917ms","start":"2024-05-05T08:47:46.724465Z","end":"2024-05-05T08:47:46.865843Z","steps":["trace[530736290] 'process raft request'  (duration: 140.411375ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-05T08:47:46.872835Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"127.639291ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/\" range_end:\"/registry/services/specs0\" ","response":"range_response_count:9 size:12938"}
{"level":"info","ts":"2024-05-05T08:47:46.872878Z","caller":"traceutil/trace.go:171","msg":"trace[1500004710] range","detail":"{range_begin:/registry/services/specs/; range_end:/registry/services/specs0; response_count:9; response_revision:11155; }","duration":"127.837792ms","start":"2024-05-05T08:47:46.745035Z","end":"2024-05-05T08:47:46.872873Z","steps":["trace[1500004710] 'agreement among raft nodes before linearized reading'  (duration: 120.638875ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T08:47:48.649811Z","caller":"traceutil/trace.go:171","msg":"trace[1059430036] linearizableReadLoop","detail":"{readStateIndex:13398; appliedIndex:13397; }","duration":"181.259042ms","start":"2024-05-05T08:47:48.468454Z","end":"2024-05-05T08:47:48.649713Z","steps":["trace[1059430036] 'read index received'  (duration: 175.976417ms)","trace[1059430036] 'applied index is now lower than readState.Index'  (duration: 5.281875ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-05T08:47:48.650539Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"181.907625ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-05-05T08:47:48.650613Z","caller":"traceutil/trace.go:171","msg":"trace[375164005] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:11155; }","duration":"182.132542ms","start":"2024-05-05T08:47:48.468448Z","end":"2024-05-05T08:47:48.650581Z","steps":["trace[375164005] 'agreement among raft nodes before linearized reading'  (duration: 181.790584ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T08:47:51.394987Z","caller":"traceutil/trace.go:171","msg":"trace[442152435] transaction","detail":"{read_only:false; response_revision:11157; number_of_response:1; }","duration":"195.317ms","start":"2024-05-05T08:47:51.199547Z","end":"2024-05-05T08:47:51.394864Z","steps":["trace[442152435] 'process raft request'  (duration: 186.477334ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T08:51:10.862754Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11074}
{"level":"info","ts":"2024-05-05T08:51:10.926803Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":11074,"took":"58.034125ms","hash":1705069602,"current-db-size-bytes":6955008,"current-db-size":"7.0 MB","current-db-size-in-use-bytes":1970176,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-05-05T08:51:10.927641Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1705069602,"revision":11074,"compact-revision":10756}
{"level":"warn","ts":"2024-05-05T08:55:19.17861Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.130708ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028962968616070 > lease_revoke:<id:70cc8f4715d4bc33>","response":"size:29"}
{"level":"info","ts":"2024-05-05T08:56:10.87071Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11364}
{"level":"info","ts":"2024-05-05T08:56:10.93389Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":11364,"took":"58.426334ms","hash":1436950314,"current-db-size-bytes":6955008,"current-db-size":"7.0 MB","current-db-size-in-use-bytes":1970176,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-05-05T08:56:10.933948Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1436950314,"revision":11364,"compact-revision":11074}
{"level":"info","ts":"2024-05-05T09:01:10.845881Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11714}
{"level":"info","ts":"2024-05-05T09:01:10.952334Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":11714,"took":"96.5425ms","hash":155083608,"current-db-size-bytes":6955008,"current-db-size":"7.0 MB","current-db-size-in-use-bytes":2748416,"current-db-size-in-use":"2.7 MB"}
{"level":"info","ts":"2024-05-05T09:01:10.952578Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":155083608,"revision":11714,"compact-revision":11364}
{"level":"info","ts":"2024-05-05T09:06:10.851799Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12139}
{"level":"info","ts":"2024-05-05T09:06:10.901706Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":12139,"took":"45.889333ms","hash":3529393533,"current-db-size-bytes":6955008,"current-db-size":"7.0 MB","current-db-size-in-use-bytes":2277376,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2024-05-05T09:06:10.901933Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3529393533,"revision":12139,"compact-revision":11714}
{"level":"info","ts":"2024-05-05T09:11:10.867164Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12378}
{"level":"info","ts":"2024-05-05T09:11:10.888109Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":12378,"took":"15.357875ms","hash":1849359046,"current-db-size-bytes":6955008,"current-db-size":"7.0 MB","current-db-size-in-use-bytes":1302528,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-05-05T09:11:10.888445Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1849359046,"revision":12378,"compact-revision":12139}
{"level":"info","ts":"2024-05-05T09:16:10.831163Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12618}
{"level":"info","ts":"2024-05-05T09:16:10.847476Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":12618,"took":"12.118833ms","hash":4174080278,"current-db-size-bytes":6955008,"current-db-size":"7.0 MB","current-db-size-in-use-bytes":1298432,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-05-05T09:16:10.847538Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4174080278,"revision":12618,"compact-revision":12378}
{"level":"info","ts":"2024-05-05T09:21:10.834873Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12858}
{"level":"info","ts":"2024-05-05T09:21:10.849254Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":12858,"took":"9.311625ms","hash":3161422643,"current-db-size-bytes":6955008,"current-db-size":"7.0 MB","current-db-size-in-use-bytes":1290240,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-05-05T09:21:10.849333Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3161422643,"revision":12858,"compact-revision":12618}
{"level":"info","ts":"2024-05-05T09:22:26.272561Z","caller":"traceutil/trace.go:171","msg":"trace[1669508127] transaction","detail":"{read_only:false; response_revision:13159; number_of_response:1; }","duration":"165.269334ms","start":"2024-05-05T09:22:25.988613Z","end":"2024-05-05T09:22:26.153882Z","steps":["trace[1669508127] 'process raft request'  (duration: 69.668875ms)","trace[1669508127] 'compare'  (duration: 91.861ms)"],"step_count":2}


==> kernel <==
 09:23:47 up  3:33,  0 users,  load average: 4.38, 4.23, 4.31
Linux minikube 6.6.12-linuxkit #1 SMP Thu Feb  8 06:36:34 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [77c375ed2c5a] <==
Trace[2146514062]: ---"Writing http response done" 192ms (08:55:05.266)
Trace[2146514062]: [812.653917ms] [812.653917ms] END
I0505 08:55:05.445801       1 trace.go:236] Trace[952145941]: "List" accept:application/json, */*,audit-id:a21dc0bc-a43d-4924-bdde-c3e869057d21,client:192.168.49.1,api-group:,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:minikube/v0.0.0 (darwin/arm64) kubernetes/$Format,verb:LIST (05-May-2024 08:55:04.847) (total time: 554ms):
Trace[952145941]: ---"Listing from storage done" 60ms (08:55:05.349)
Trace[952145941]: ---"Writing http response done" count:9 52ms (08:55:05.401)
Trace[952145941]: [554.610333ms] [554.610333ms] END
I0505 08:55:19.466709       1 trace.go:236] Trace[1198200454]: "Patch" accept:application/json, */*,audit-id:b44ce1bb-3b92-4dc2-917b-9085eddceb51,client:10.244.0.52,api-group:,api-version:v1,name:ingress-nginx-controller-6d675964ff-tgd77.17cc8bcd343a1748,subresource:,namespace:ingress-nginx,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/ingress-nginx/events/ingress-nginx-controller-6d675964ff-tgd77.17cc8bcd343a1748,user-agent:nginx-ingress-controller/v1.10.1 (linux/arm64) ingress-nginx/4fb5aac1dd3669daa3a14d9de3e3cdb371b4c518,verb:PATCH (05-May-2024 08:55:17.084) (total time: 2379ms):
Trace[1198200454]: ["GuaranteedUpdate etcd3" audit-id:b44ce1bb-3b92-4dc2-917b-9085eddceb51,key:/events/ingress-nginx/ingress-nginx-controller-6d675964ff-tgd77.17cc8bcd343a1748,type:*core.Event,resource:events 1902ms (08:55:17.561)
Trace[1198200454]:  ---"initial value restored" 1797ms (08:55:19.359)
Trace[1198200454]:  ---"Txn call completed" 58ms (08:55:19.451)]
Trace[1198200454]: ---"About to check admission control" 21ms (08:55:19.383)
Trace[1198200454]: ---"Object stored in database" 75ms (08:55:19.458)
Trace[1198200454]: [2.379578627s] [2.379578627s] END
I0505 08:55:21.762392       1 trace.go:236] Trace[318235548]: "Create" accept:application/vnd.kubernetes.protobuf, */*,audit-id:82b3d9fa-4316-4f9e-9571-1c549536f088,client:192.168.49.2,api-group:,api-version:v1,name:,subresource:,namespace:ingress-nginx,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/ingress-nginx/events,user-agent:kube-scheduler/v1.30.0 (linux/arm64) kubernetes/7c48c2b/scheduler,verb:POST (05-May-2024 08:55:20.983) (total time: 767ms):
Trace[318235548]: ---"Conversion done" 64ms (08:55:21.050)
Trace[318235548]: ---"Writing http response done" 73ms (08:55:21.750)
Trace[318235548]: [767.688834ms] [767.688834ms] END
I0505 08:55:21.762385       1 trace.go:236] Trace[5672019]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:8652bd53-804b-4098-9a37-c0507fd7abe0,client:192.168.49.2,api-group:,api-version:v1,name:ingress-nginx-controller-admission,subresource:,namespace:ingress-nginx,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/ingress-nginx/endpoints/ingress-nginx-controller-admission,user-agent:kube-controller-manager/v1.30.0 (linux/arm64) kubernetes/7c48c2b/system:serviceaccount:kube-system:endpoint-controller,verb:PUT (05-May-2024 08:55:21.196) (total time: 559ms):
Trace[5672019]: ---"limitedReadBody succeeded" len:272 73ms (08:55:21.269)
Trace[5672019]: [559.696875ms] [559.696875ms] END
I0505 08:55:21.762421       1 trace.go:236] Trace[2089890993]: "Create" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ee80afdd-7747-4928-b7f3-58901c8f695d,client:192.168.49.2,api-group:,api-version:v1,name:,subresource:,namespace:ingress-nginx,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/ingress-nginx/events,user-agent:kube-controller-manager/v1.30.0 (linux/arm64) kubernetes/7c48c2b/system:serviceaccount:kube-system:replicaset-controller,verb:POST (05-May-2024 08:55:21.196) (total time: 554ms):
Trace[2089890993]: ---"About to convert to expected version" 170ms (08:55:21.372)
Trace[2089890993]: ---"Writing http response done" 55ms (08:55:21.750)
Trace[2089890993]: [554.267292ms] [554.267292ms] END
I0505 08:55:21.762905       1 trace.go:236] Trace[991118379]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a1317fdf-7049-4201-8329-b5f883502992,client:192.168.49.2,api-group:,api-version:v1,name:ingress-nginx-controller,subresource:,namespace:ingress-nginx,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/ingress-nginx/endpoints/ingress-nginx-controller,user-agent:kube-controller-manager/v1.30.0 (linux/arm64) kubernetes/7c48c2b/system:serviceaccount:kube-system:endpoint-controller,verb:PUT (05-May-2024 08:55:21.196) (total time: 566ms):
Trace[991118379]: ---"Conversion done" 163ms (08:55:21.365)
Trace[991118379]: [566.259292ms] [566.259292ms] END
I0505 08:55:21.866893       1 trace.go:236] Trace[1796838288]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:c38bfc7c-72f9-4b37-bade-0eee4510d937,client:192.168.49.2,api-group:discovery.k8s.io,api-version:v1,name:ingress-nginx-controller-tsrkg,subresource:,namespace:ingress-nginx,protocol:HTTP/2.0,resource:endpointslices,scope:resource,url:/apis/discovery.k8s.io/v1/namespaces/ingress-nginx/endpointslices/ingress-nginx-controller-tsrkg,user-agent:kube-controller-manager/v1.30.0 (linux/arm64) kubernetes/7c48c2b/system:serviceaccount:kube-system:endpointslice-controller,verb:PUT (05-May-2024 08:55:21.196) (total time: 652ms):
Trace[1796838288]: ---"limitedReadBody succeeded" len:722 289ms (08:55:21.486)
Trace[1796838288]: ---"Conversion done" 79ms (08:55:21.569)
Trace[1796838288]: ---"Writing http response done" 78ms (08:55:21.848)
Trace[1796838288]: [652.82275ms] [652.82275ms] END
I0505 08:55:21.971043       1 trace.go:236] Trace[773656369]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b460a9d2-7adb-4756-b738-25a687900ca1,client:192.168.49.2,api-group:,api-version:v1,name:ingress-nginx-controller-84df5799c-bkzl4,subresource:status,namespace:ingress-nginx,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/ingress-nginx/pods/ingress-nginx-controller-84df5799c-bkzl4/status,user-agent:kube-scheduler/v1.30.0 (linux/arm64) kubernetes/7c48c2b/scheduler,verb:PATCH (05-May-2024 08:55:20.982) (total time: 976ms):
Trace[773656369]: ["GuaranteedUpdate etcd3" audit-id:b460a9d2-7adb-4756-b738-25a687900ca1,key:/pods/ingress-nginx/ingress-nginx-controller-84df5799c-bkzl4,type:*core.Pod,resource:pods 883ms (08:55:21.077)
Trace[773656369]:  ---"initial value restored" 68ms (08:55:21.145)
Trace[773656369]:  ---"About to Encode" 612ms (08:55:21.758)]
Trace[773656369]: ---"About to check admission control" 600ms (08:55:21.749)
Trace[773656369]: ---"Object stored in database" 15ms (08:55:21.765)
Trace[773656369]: ---"Writing http response done" 194ms (08:55:21.959)
Trace[773656369]: [976.889ms] [976.889ms] END
I0505 08:55:21.987714       1 trace.go:236] Trace[39781896]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:fc86bf68-4353-4ee1-96a0-9b14d207a664,client:192.168.49.2,api-group:discovery.k8s.io,api-version:v1,name:ingress-nginx-controller-admission-d4bqm,subresource:,namespace:ingress-nginx,protocol:HTTP/2.0,resource:endpointslices,scope:resource,url:/apis/discovery.k8s.io/v1/namespaces/ingress-nginx/endpointslices/ingress-nginx-controller-admission-d4bqm,user-agent:kube-controller-manager/v1.30.0 (linux/arm64) kubernetes/7c48c2b/system:serviceaccount:kube-system:endpointslice-controller,verb:PUT (05-May-2024 08:55:21.196) (total time: 787ms):
Trace[39781896]: ---"limitedReadBody succeeded" len:749 166ms (08:55:21.363)
Trace[39781896]: ---"Write to database call succeeded" len:749 70ms (08:55:21.948)
Trace[39781896]: ---"Writing http response done" 35ms (08:55:21.983)
Trace[39781896]: [787.19325ms] [787.19325ms] END
I0505 08:55:22.858649       1 trace.go:236] Trace[115679793]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4d3a1b3e-7686-4f06-9492-10d96bd550bd,client:192.168.49.2,api-group:apps,api-version:v1,name:ingress-nginx-controller,subresource:status,namespace:ingress-nginx,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/ingress-nginx/deployments/ingress-nginx-controller/status,user-agent:kube-controller-manager/v1.30.0 (linux/arm64) kubernetes/7c48c2b/system:serviceaccount:kube-system:deployment-controller,verb:PUT (05-May-2024 08:55:21.672) (total time: 1173ms):
Trace[115679793]: ---"limitedReadBody succeeded" len:5229 72ms (08:55:21.744)
Trace[115679793]: ["GuaranteedUpdate etcd3" audit-id:4d3a1b3e-7686-4f06-9492-10d96bd550bd,key:/deployments/ingress-nginx/ingress-nginx-controller,type:*apps.Deployment,resource:deployments.apps 1072ms (08:55:21.775)
Trace[115679793]:  ---"initial value restored" 198ms (08:55:21.974)
Trace[115679793]:  ---"About to Encode" 707ms (08:55:22.681)
Trace[115679793]:  ---"decode succeeded" len:8423 58ms (08:55:22.754)]
Trace[115679793]: ---"Writing http response done" 90ms (08:55:22.846)
Trace[115679793]: [1.173599126s] [1.173599126s] END
I0505 08:55:23.654835       1 trace.go:236] Trace[2037425767]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:8bd0d188-f09e-4488-b4e0-9fa41d76f4bb,client:192.168.49.2,api-group:apps,api-version:v1,name:ingress-nginx-controller,subresource:status,namespace:ingress-nginx,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/ingress-nginx/deployments/ingress-nginx-controller/status,user-agent:kube-controller-manager/v1.30.0 (linux/arm64) kubernetes/7c48c2b/system:serviceaccount:kube-system:deployment-controller,verb:PUT (05-May-2024 08:55:23.061) (total time: 510ms):
Trace[2037425767]: [510.467541ms] [510.467541ms] END
I0505 08:55:23.746813       1 trace.go:236] Trace[936625488]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:eb5e3de8-8473-4835-9e7f-2d0eed01388f,client:192.168.49.2,api-group:apps,api-version:v1,name:ingress-nginx-controller-6d675964ff,subresource:status,namespace:ingress-nginx,protocol:HTTP/2.0,resource:replicasets,scope:resource,url:/apis/apps/v1/namespaces/ingress-nginx/replicasets/ingress-nginx-controller-6d675964ff/status,user-agent:kube-controller-manager/v1.30.0 (linux/arm64) kubernetes/7c48c2b/system:serviceaccount:kube-system:replicaset-controller,verb:PUT (05-May-2024 08:55:23.185) (total time: 561ms):
Trace[936625488]: [561.044542ms] [561.044542ms] END
I0505 08:58:27.881009       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs={"IPv4":"10.106.241.229"}
I0505 08:58:27.909898       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs={"IPv4":"10.107.104.56"}
W0505 08:58:56.882287       1 dispatcher.go:217] Failed calling webhook, failing closed validate.nginx.ingress.kubernetes.io: failed calling webhook "validate.nginx.ingress.kubernetes.io": failed to call webhook: Post "https://ingress-nginx-controller-admission.ingress-nginx.svc:443/networking/v1/ingresses?timeout=10s": dial tcp 10.107.104.56:443: connect: connection refused


==> kube-controller-manager [e45ed06e16f6] <==
I0505 08:55:25.173531       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:55:25.207443       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:55:25.209361       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:55:25.216684       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:55:25.220394       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:55:25.243847       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:55:25.249913       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:55:25.285924       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:55:25.289112       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:55:25.357831       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:55:25.369776       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:55:34.684097       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-6d675964ff" duration="3.719417ms"
I0505 08:55:35.475370       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-6d675964ff" duration="1.812ms"
I0505 08:55:35.503804       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-6d675964ff" duration="349.041µs"
I0505 08:55:35.510381       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-6d675964ff" duration="156.875µs"
I0505 08:56:09.686502       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:56:09.688748       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:56:10.941966       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:56:10.947339       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:56:11.707435       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:56:11.716374       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:56:12.068184       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:56:12.068678       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:56:12.180316       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:56:12.180584       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:56:12.249972       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:56:12.250534       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:57:42.746502       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:57:42.747462       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:57:42.750844       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-6d675964ff" duration="3.875µs"
I0505 08:57:42.750909       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-84df5799c" duration="5.042µs"
I0505 08:57:52.665305       1 namespace_controller.go:182] "Namespace has been deleted" logger="namespace-controller" namespace="ingress-nginx"
I0505 08:58:27.969323       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:58:27.970250       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:58:28.000889       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:58:28.007894       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:58:28.055903       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:58:28.061657       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:58:28.063883       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-84df5799c" duration="80.372583ms"
I0505 08:58:28.071026       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-84df5799c" duration="7.098041ms"
I0505 08:58:28.071155       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-84df5799c" duration="94.417µs"
I0505 08:58:28.113048       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:58:28.113988       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:58:28.165275       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-84df5799c" duration="73.542µs"
I0505 08:58:28.272161       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:58:28.470706       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:58:30.161241       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:58:30.182457       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:58:31.209524       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:58:31.347173       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:58:32.268037       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:58:32.288848       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:58:32.371791       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:58:32.381755       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:58:32.388120       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0505 08:58:32.444792       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:58:33.315148       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:58:33.347414       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:58:33.363916       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0505 08:58:33.373509       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"


==> kube-proxy [c3ceae517074] <==
I0505 04:49:47.779580       1 server_linux.go:69] "Using iptables proxy"
I0505 04:49:47.876180       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0505 04:49:47.954353       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0505 04:49:47.954405       1 server_linux.go:165] "Using iptables Proxier"
I0505 04:49:47.955527       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0505 04:49:47.955547       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0505 04:49:47.955623       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0505 04:49:47.956365       1 server.go:872] "Version info" version="v1.30.0"
I0505 04:49:47.956380       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0505 04:49:47.958991       1 config.go:192] "Starting service config controller"
I0505 04:49:47.959100       1 config.go:101] "Starting endpoint slice config controller"
I0505 04:49:47.959563       1 shared_informer.go:313] Waiting for caches to sync for service config
I0505 04:49:47.959571       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0505 04:49:47.959628       1 config.go:319] "Starting node config controller"
I0505 04:49:47.959632       1 shared_informer.go:313] Waiting for caches to sync for node config
I0505 04:49:48.060441       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0505 04:49:48.060441       1 shared_informer.go:320] Caches are synced for node config
I0505 04:49:48.060477       1 shared_informer.go:320] Caches are synced for service config
I0505 08:38:41.570100       1 trace.go:236] Trace[2011927809]: "iptables ChainExists" (05-May-2024 08:29:01.741) (total time: 4876ms):
Trace[2011927809]: [4.876168085s] [4.876168085s] END
I0505 08:38:41.862938       1 trace.go:236] Trace[979370622]: "iptables ChainExists" (05-May-2024 08:29:01.818) (total time: 6199ms):
Trace[979370622]: [6.199342211s] [6.199342211s] END
I0505 08:38:55.568237       1 trace.go:236] Trace[726801333]: "iptables save" (05-May-2024 08:38:53.174) (total time: 2306ms):
Trace[726801333]: [2.306340085s] [2.306340085s] END


==> kube-scheduler [0b63c8528a8f] <==
I0505 04:49:24.483812       1 serving.go:380] Generated self-signed cert in-memory
W0505 04:49:27.679476       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0505 04:49:27.684242       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0505 04:49:27.684279       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0505 04:49:27.684294       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0505 04:49:28.019077       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0505 04:49:28.019122       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0505 04:49:28.069958       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0505 04:49:28.070058       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0505 04:49:28.070093       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0505 04:49:28.070381       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0505 04:49:28.077172       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0505 04:49:28.077228       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0505 04:49:28.081016       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0505 04:49:28.081029       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0505 04:49:28.082968       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0505 04:49:28.082984       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0505 04:49:28.126587       1 reflector.go:547] runtime/asm_arm64.s:1222: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0505 04:49:28.126608       1 reflector.go:150] runtime/asm_arm64.s:1222: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0505 04:49:28.131200       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0505 04:49:28.131214       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0505 04:49:28.133199       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0505 04:49:28.133212       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0505 04:49:28.135485       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0505 04:49:28.135526       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0505 04:49:28.137837       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0505 04:49:28.137854       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0505 04:49:28.137915       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0505 04:49:28.137925       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0505 04:49:28.140146       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0505 04:49:28.140173       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0505 04:49:28.142380       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0505 04:49:28.142425       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0505 04:49:28.169534       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0505 04:49:28.170298       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0505 04:49:28.170515       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0505 04:49:28.172807       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0505 04:49:28.169577       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0505 04:49:28.175009       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0505 04:49:28.170550       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0505 04:49:28.175026       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
I0505 04:49:29.471667       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
May 05 08:54:31 minikube kubelet[2215]: I0505 08:54:31.187584    2215 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="ingress-nginx/ingress-nginx-controller-6d675964ff-tgd77" podStartSLOduration=377.346322193 podStartE2EDuration="8m42.172545051s" podCreationTimestamp="2024-05-05 08:45:49 +0000 UTC" firstStartedPulling="2024-05-05 08:52:02.747747094 +0000 UTC m=+10847.974383852" lastFinishedPulling="2024-05-05 08:54:27.575478633 +0000 UTC m=+10992.800606710" observedRunningTime="2024-05-05 08:54:31.124451134 +0000 UTC m=+10996.349579212" watchObservedRunningTime="2024-05-05 08:54:31.172545051 +0000 UTC m=+10996.397673170"
May 05 08:55:25 minikube kubelet[2215]: I0505 08:55:25.291822    2215 topology_manager.go:215] "Topology Admit Handler" podUID="24486595-24bb-439a-8f01-b147af910bce" podNamespace="ingress-nginx" podName="ingress-nginx-admission-create-nkztc"
May 05 08:55:25 minikube kubelet[2215]: E0505 08:55:25.293760    2215 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="b75e420d-fec4-4444-9e36-823c5e1e1672" containerName="patch"
May 05 08:55:25 minikube kubelet[2215]: E0505 08:55:25.293809    2215 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="b75e420d-fec4-4444-9e36-823c5e1e1672" containerName="patch"
May 05 08:55:25 minikube kubelet[2215]: E0505 08:55:25.293821    2215 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="9a548609-b39e-4e52-9401-70a928759b04" containerName="create"
May 05 08:55:25 minikube kubelet[2215]: I0505 08:55:25.294734    2215 memory_manager.go:354] "RemoveStaleState removing state" podUID="9a548609-b39e-4e52-9401-70a928759b04" containerName="create"
May 05 08:55:25 minikube kubelet[2215]: I0505 08:55:25.294753    2215 memory_manager.go:354] "RemoveStaleState removing state" podUID="b75e420d-fec4-4444-9e36-823c5e1e1672" containerName="patch"
May 05 08:55:25 minikube kubelet[2215]: I0505 08:55:25.298177    2215 topology_manager.go:215] "Topology Admit Handler" podUID="7740877b-bbd8-4267-b3e1-47ec761cbda5" podNamespace="ingress-nginx" podName="ingress-nginx-admission-patch-c6l8s"
May 05 08:55:25 minikube kubelet[2215]: I0505 08:55:25.298291    2215 memory_manager.go:354] "RemoveStaleState removing state" podUID="b75e420d-fec4-4444-9e36-823c5e1e1672" containerName="patch"
May 05 08:55:25 minikube kubelet[2215]: I0505 08:55:25.471792    2215 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-8zxrq\" (UniqueName: \"kubernetes.io/projected/7740877b-bbd8-4267-b3e1-47ec761cbda5-kube-api-access-8zxrq\") pod \"ingress-nginx-admission-patch-c6l8s\" (UID: \"7740877b-bbd8-4267-b3e1-47ec761cbda5\") " pod="ingress-nginx/ingress-nginx-admission-patch-c6l8s"
May 05 08:55:25 minikube kubelet[2215]: I0505 08:55:25.472054    2215 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-r4bph\" (UniqueName: \"kubernetes.io/projected/24486595-24bb-439a-8f01-b147af910bce-kube-api-access-r4bph\") pod \"ingress-nginx-admission-create-nkztc\" (UID: \"24486595-24bb-439a-8f01-b147af910bce\") " pod="ingress-nginx/ingress-nginx-admission-create-nkztc"
May 05 08:55:26 minikube kubelet[2215]: I0505 08:55:26.188732    2215 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="1758c3cb5e36064990828a69cd6e0f396db2491ba376465530e9de34045e7ee4"
May 05 08:55:26 minikube kubelet[2215]: I0505 08:55:26.190704    2215 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="83d5d0666ca43884fcfa8639ad5842bc060e9867b2d1e051464c09d43cd394c3"
May 05 08:55:26 minikube kubelet[2215]: I0505 08:55:26.953786    2215 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="9a548609-b39e-4e52-9401-70a928759b04" path="/var/lib/kubelet/pods/9a548609-b39e-4e52-9401-70a928759b04/volumes"
May 05 08:55:26 minikube kubelet[2215]: I0505 08:55:26.957909    2215 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="b75e420d-fec4-4444-9e36-823c5e1e1672" path="/var/lib/kubelet/pods/b75e420d-fec4-4444-9e36-823c5e1e1672/volumes"
May 05 08:55:34 minikube kubelet[2215]: I0505 08:55:34.825742    2215 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-rvhwb\" (UniqueName: \"kubernetes.io/projected/1480fe77-2d23-495e-ad67-993d7e6d45c0-kube-api-access-rvhwb\") pod \"1480fe77-2d23-495e-ad67-993d7e6d45c0\" (UID: \"1480fe77-2d23-495e-ad67-993d7e6d45c0\") "
May 05 08:55:34 minikube kubelet[2215]: I0505 08:55:34.825967    2215 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/1480fe77-2d23-495e-ad67-993d7e6d45c0-webhook-cert\") pod \"1480fe77-2d23-495e-ad67-993d7e6d45c0\" (UID: \"1480fe77-2d23-495e-ad67-993d7e6d45c0\") "
May 05 08:55:34 minikube kubelet[2215]: I0505 08:55:34.832571    2215 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/1480fe77-2d23-495e-ad67-993d7e6d45c0-kube-api-access-rvhwb" (OuterVolumeSpecName: "kube-api-access-rvhwb") pod "1480fe77-2d23-495e-ad67-993d7e6d45c0" (UID: "1480fe77-2d23-495e-ad67-993d7e6d45c0"). InnerVolumeSpecName "kube-api-access-rvhwb". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 05 08:55:34 minikube kubelet[2215]: I0505 08:55:34.832571    2215 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/1480fe77-2d23-495e-ad67-993d7e6d45c0-webhook-cert" (OuterVolumeSpecName: "webhook-cert") pod "1480fe77-2d23-495e-ad67-993d7e6d45c0" (UID: "1480fe77-2d23-495e-ad67-993d7e6d45c0"). InnerVolumeSpecName "webhook-cert". PluginName "kubernetes.io/secret", VolumeGidValue ""
May 05 08:55:34 minikube kubelet[2215]: I0505 08:55:34.931147    2215 reconciler_common.go:289] "Volume detached for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/1480fe77-2d23-495e-ad67-993d7e6d45c0-webhook-cert\") on node \"minikube\" DevicePath \"\""
May 05 08:55:34 minikube kubelet[2215]: I0505 08:55:34.932002    2215 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-rvhwb\" (UniqueName: \"kubernetes.io/projected/1480fe77-2d23-495e-ad67-993d7e6d45c0-kube-api-access-rvhwb\") on node \"minikube\" DevicePath \"\""
May 05 08:55:35 minikube kubelet[2215]: I0505 08:55:35.455046    2215 scope.go:117] "RemoveContainer" containerID="3f7ee71e3ccfe7a15d954c9208b968889cdebbb67c55614de8609af834bd81fe"
May 05 08:55:35 minikube kubelet[2215]: I0505 08:55:35.499322    2215 scope.go:117] "RemoveContainer" containerID="3f7ee71e3ccfe7a15d954c9208b968889cdebbb67c55614de8609af834bd81fe"
May 05 08:55:35 minikube kubelet[2215]: E0505 08:55:35.500568    2215 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 3f7ee71e3ccfe7a15d954c9208b968889cdebbb67c55614de8609af834bd81fe" containerID="3f7ee71e3ccfe7a15d954c9208b968889cdebbb67c55614de8609af834bd81fe"
May 05 08:55:35 minikube kubelet[2215]: I0505 08:55:35.502443    2215 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"3f7ee71e3ccfe7a15d954c9208b968889cdebbb67c55614de8609af834bd81fe"} err="failed to get container status \"3f7ee71e3ccfe7a15d954c9208b968889cdebbb67c55614de8609af834bd81fe\": rpc error: code = Unknown desc = Error response from daemon: No such container: 3f7ee71e3ccfe7a15d954c9208b968889cdebbb67c55614de8609af834bd81fe"
May 05 08:55:36 minikube kubelet[2215]: I0505 08:55:36.974408    2215 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="1480fe77-2d23-495e-ad67-993d7e6d45c0" path="/var/lib/kubelet/pods/1480fe77-2d23-495e-ad67-993d7e6d45c0/volumes"
May 05 08:55:43 minikube kubelet[2215]: I0505 08:55:43.433824    2215 scope.go:117] "RemoveContainer" containerID="98fc446c9b790b8ae5856ed7664366b37e11a49c94ea2038ac7e64e4c026307a"
May 05 08:55:43 minikube kubelet[2215]: I0505 08:55:43.461948    2215 scope.go:117] "RemoveContainer" containerID="8f20c421061da72efefb427738adba9b6d03b9173a0175544360e63555c52483"
May 05 08:56:10 minikube kubelet[2215]: I0505 08:56:10.986961    2215 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-8zxrq\" (UniqueName: \"kubernetes.io/projected/7740877b-bbd8-4267-b3e1-47ec761cbda5-kube-api-access-8zxrq\") pod \"7740877b-bbd8-4267-b3e1-47ec761cbda5\" (UID: \"7740877b-bbd8-4267-b3e1-47ec761cbda5\") "
May 05 08:56:10 minikube kubelet[2215]: I0505 08:56:10.987110    2215 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-r4bph\" (UniqueName: \"kubernetes.io/projected/24486595-24bb-439a-8f01-b147af910bce-kube-api-access-r4bph\") pod \"24486595-24bb-439a-8f01-b147af910bce\" (UID: \"24486595-24bb-439a-8f01-b147af910bce\") "
May 05 08:56:10 minikube kubelet[2215]: I0505 08:56:10.992881    2215 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/7740877b-bbd8-4267-b3e1-47ec761cbda5-kube-api-access-8zxrq" (OuterVolumeSpecName: "kube-api-access-8zxrq") pod "7740877b-bbd8-4267-b3e1-47ec761cbda5" (UID: "7740877b-bbd8-4267-b3e1-47ec761cbda5"). InnerVolumeSpecName "kube-api-access-8zxrq". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 05 08:56:10 minikube kubelet[2215]: I0505 08:56:10.993672    2215 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/24486595-24bb-439a-8f01-b147af910bce-kube-api-access-r4bph" (OuterVolumeSpecName: "kube-api-access-r4bph") pod "24486595-24bb-439a-8f01-b147af910bce" (UID: "24486595-24bb-439a-8f01-b147af910bce"). InnerVolumeSpecName "kube-api-access-r4bph". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 05 08:56:11 minikube kubelet[2215]: I0505 08:56:11.089239    2215 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-r4bph\" (UniqueName: \"kubernetes.io/projected/24486595-24bb-439a-8f01-b147af910bce-kube-api-access-r4bph\") on node \"minikube\" DevicePath \"\""
May 05 08:56:11 minikube kubelet[2215]: I0505 08:56:11.089290    2215 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-8zxrq\" (UniqueName: \"kubernetes.io/projected/7740877b-bbd8-4267-b3e1-47ec761cbda5-kube-api-access-8zxrq\") on node \"minikube\" DevicePath \"\""
May 05 08:56:11 minikube kubelet[2215]: I0505 08:56:11.690652    2215 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="83d5d0666ca43884fcfa8639ad5842bc060e9867b2d1e051464c09d43cd394c3"
May 05 08:56:11 minikube kubelet[2215]: I0505 08:56:11.699101    2215 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="1758c3cb5e36064990828a69cd6e0f396db2491ba376465530e9de34045e7ee4"
May 05 08:57:42 minikube kubelet[2215]: I0505 08:57:42.921877    2215 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="24486595-24bb-439a-8f01-b147af910bce" path="/var/lib/kubelet/pods/24486595-24bb-439a-8f01-b147af910bce/volumes"
May 05 08:57:42 minikube kubelet[2215]: I0505 08:57:42.924473    2215 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="7740877b-bbd8-4267-b3e1-47ec761cbda5" path="/var/lib/kubelet/pods/7740877b-bbd8-4267-b3e1-47ec761cbda5/volumes"
May 05 08:57:43 minikube kubelet[2215]: I0505 08:57:43.602820    2215 scope.go:117] "RemoveContainer" containerID="6a7f45ee259dc0e1652e072a93cacf15f614fa23e06156f6d8f3a5be1f4b6b6d"
May 05 08:57:43 minikube kubelet[2215]: I0505 08:57:43.633037    2215 scope.go:117] "RemoveContainer" containerID="15488b7025dd3061600b387481c2dc3be1362cc1cc98df8f05c2f6b54cd2be3c"
May 05 08:58:28 minikube kubelet[2215]: I0505 08:58:28.189162    2215 topology_manager.go:215] "Topology Admit Handler" podUID="f2571a29-f14c-4b17-887a-0e1bfba14a5d" podNamespace="ingress-nginx" podName="ingress-nginx-admission-create-hq5qj"
May 05 08:58:28 minikube kubelet[2215]: E0505 08:58:28.192362    2215 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="1480fe77-2d23-495e-ad67-993d7e6d45c0" containerName="controller"
May 05 08:58:28 minikube kubelet[2215]: E0505 08:58:28.192519    2215 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="7740877b-bbd8-4267-b3e1-47ec761cbda5" containerName="patch"
May 05 08:58:28 minikube kubelet[2215]: E0505 08:58:28.192530    2215 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="24486595-24bb-439a-8f01-b147af910bce" containerName="create"
May 05 08:58:28 minikube kubelet[2215]: I0505 08:58:28.193383    2215 memory_manager.go:354] "RemoveStaleState removing state" podUID="7740877b-bbd8-4267-b3e1-47ec761cbda5" containerName="patch"
May 05 08:58:28 minikube kubelet[2215]: I0505 08:58:28.193408    2215 memory_manager.go:354] "RemoveStaleState removing state" podUID="24486595-24bb-439a-8f01-b147af910bce" containerName="create"
May 05 08:58:28 minikube kubelet[2215]: I0505 08:58:28.193411    2215 memory_manager.go:354] "RemoveStaleState removing state" podUID="1480fe77-2d23-495e-ad67-993d7e6d45c0" containerName="controller"
May 05 08:58:28 minikube kubelet[2215]: I0505 08:58:28.200056    2215 topology_manager.go:215] "Topology Admit Handler" podUID="fc385fd3-427e-4da1-8389-a4ad3ba841e1" podNamespace="ingress-nginx" podName="ingress-nginx-admission-patch-6vbwp"
May 05 08:58:28 minikube kubelet[2215]: I0505 08:58:28.253981    2215 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6kgd8\" (UniqueName: \"kubernetes.io/projected/fc385fd3-427e-4da1-8389-a4ad3ba841e1-kube-api-access-6kgd8\") pod \"ingress-nginx-admission-patch-6vbwp\" (UID: \"fc385fd3-427e-4da1-8389-a4ad3ba841e1\") " pod="ingress-nginx/ingress-nginx-admission-patch-6vbwp"
May 05 08:58:28 minikube kubelet[2215]: I0505 08:58:28.254177    2215 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-s7cn7\" (UniqueName: \"kubernetes.io/projected/f2571a29-f14c-4b17-887a-0e1bfba14a5d-kube-api-access-s7cn7\") pod \"ingress-nginx-admission-create-hq5qj\" (UID: \"f2571a29-f14c-4b17-887a-0e1bfba14a5d\") " pod="ingress-nginx/ingress-nginx-admission-create-hq5qj"
May 05 08:58:30 minikube kubelet[2215]: I0505 08:58:30.127651    2215 scope.go:117] "RemoveContainer" containerID="2e6cc4eae58dfa4408b72fe93efcf7c41a54aa958a6da80b9cf06f15128ba57a"
May 05 08:58:31 minikube kubelet[2215]: I0505 08:58:31.183725    2215 scope.go:117] "RemoveContainer" containerID="2e6cc4eae58dfa4408b72fe93efcf7c41a54aa958a6da80b9cf06f15128ba57a"
May 05 08:58:31 minikube kubelet[2215]: I0505 08:58:31.487471    2215 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-s7cn7\" (UniqueName: \"kubernetes.io/projected/f2571a29-f14c-4b17-887a-0e1bfba14a5d-kube-api-access-s7cn7\") pod \"f2571a29-f14c-4b17-887a-0e1bfba14a5d\" (UID: \"f2571a29-f14c-4b17-887a-0e1bfba14a5d\") "
May 05 08:58:31 minikube kubelet[2215]: I0505 08:58:31.492297    2215 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/f2571a29-f14c-4b17-887a-0e1bfba14a5d-kube-api-access-s7cn7" (OuterVolumeSpecName: "kube-api-access-s7cn7") pod "f2571a29-f14c-4b17-887a-0e1bfba14a5d" (UID: "f2571a29-f14c-4b17-887a-0e1bfba14a5d"). InnerVolumeSpecName "kube-api-access-s7cn7". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 05 08:58:31 minikube kubelet[2215]: I0505 08:58:31.588365    2215 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-s7cn7\" (UniqueName: \"kubernetes.io/projected/f2571a29-f14c-4b17-887a-0e1bfba14a5d-kube-api-access-s7cn7\") on node \"minikube\" DevicePath \"\""
May 05 08:58:32 minikube kubelet[2215]: I0505 08:58:32.262807    2215 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e90552a00ab60abb741c849c449a42ea276250fc4f328cd3edb83b868f02f148"
May 05 08:58:32 minikube kubelet[2215]: I0505 08:58:32.548103    2215 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-6kgd8\" (UniqueName: \"kubernetes.io/projected/fc385fd3-427e-4da1-8389-a4ad3ba841e1-kube-api-access-6kgd8\") pod \"fc385fd3-427e-4da1-8389-a4ad3ba841e1\" (UID: \"fc385fd3-427e-4da1-8389-a4ad3ba841e1\") "
May 05 08:58:32 minikube kubelet[2215]: I0505 08:58:32.552134    2215 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/fc385fd3-427e-4da1-8389-a4ad3ba841e1-kube-api-access-6kgd8" (OuterVolumeSpecName: "kube-api-access-6kgd8") pod "fc385fd3-427e-4da1-8389-a4ad3ba841e1" (UID: "fc385fd3-427e-4da1-8389-a4ad3ba841e1"). InnerVolumeSpecName "kube-api-access-6kgd8". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 05 08:58:32 minikube kubelet[2215]: I0505 08:58:32.649925    2215 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-6kgd8\" (UniqueName: \"kubernetes.io/projected/fc385fd3-427e-4da1-8389-a4ad3ba841e1-kube-api-access-6kgd8\") on node \"minikube\" DevicePath \"\""
May 05 08:58:33 minikube kubelet[2215]: I0505 08:58:33.315968    2215 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3404a2975a232f8a35f1a67f93978898eda7b1b4c898d97e3a30f6750040a9ce"


==> storage-provisioner [198e788276ad] <==
I0505 08:38:47.981130       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0505 08:38:48.068565       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0505 08:38:48.068981       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0505 08:39:03.773697       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0505 08:39:03.774750       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"1897cf83-7859-412c-a26f-2263daa5ba1e", APIVersion:"v1", ResourceVersion:"10523", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_b8e1c5ac-375c-430d-98cb-0d68d85fa2c1 became leader
I0505 08:39:03.775185       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_b8e1c5ac-375c-430d-98cb-0d68d85fa2c1!
I0505 08:39:03.878619       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_b8e1c5ac-375c-430d-98cb-0d68d85fa2c1!


==> storage-provisioner [b36a1c9081d5] <==
encoding/json.(*Decoder).Decode(0x40005bb080, 0xefe300, 0x400065e438, 0x0, 0x0)
	/usr/local/go/src/encoding/json/stream.go:63 +0x60
k8s.io/apimachinery/pkg/util/framer.(*jsonFrameReader).Read(0x40002e3650, 0x40004a6400, 0x400, 0x400, 0x4000073d00, 0x15400, 0x4000073d00)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/framer/framer.go:152 +0x194
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0x4000211630, 0x0, 0x126f7a8, 0x40001ce7c0, 0x0, 0x0, 0x6cad0, 0x40001610e0, 0x400022e648)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x70
k8s.io/client-go/rest/watch.(*Decoder).Decode(0x400002a5e0, 0x4000073ef0, 0x8, 0x126f1e0, 0x40006eef00, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x60
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0x400062e6c0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0xe8
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xb4

goroutine 12620 [sync.Cond.Wait]:
sync.runtime_notifyListWait(0x4000134880, 0x4000000000)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0x4000134870)
	/usr/local/go/src/sync/cond.go:56 +0xb8
golang.org/x/net/http2.(*pipe).Read(0x4000134868, 0x40007e6400, 0x200, 0x200, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/pipe.go:65 +0x94
golang.org/x/net/http2.transportResponseBody.Read(0x4000134840, 0x40007e6400, 0x200, 0x200, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/transport.go:2108 +0x80
encoding/json.(*Decoder).refill(0x4000134c60, 0x4000700200, 0x400066bc48)
	/usr/local/go/src/encoding/json/stream.go:165 +0xd4
encoding/json.(*Decoder).readValue(0x4000134c60, 0x0, 0x0, 0x7952ac)
	/usr/local/go/src/encoding/json/stream.go:140 +0x1b8
encoding/json.(*Decoder).Decode(0x4000134c60, 0xefe300, 0x40001a9a10, 0x12664e8, 0x4000198060)
	/usr/local/go/src/encoding/json/stream.go:63 +0x60
k8s.io/apimachinery/pkg/util/framer.(*jsonFrameReader).Read(0x40002c5dd0, 0x40007e8800, 0x400, 0x400, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/framer/framer.go:152 +0x194
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0x4000195c20, 0x0, 0x126f7a8, 0x4000049340, 0x3, 0x0, 0x11530b0, 0x0, 0x40004b3e48)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x70
k8s.io/client-go/rest/watch.(*Decoder).Decode(0x40008c6e00, 0x1153078, 0x0, 0x0, 0x40004b3f1e, 0x2, 0x3)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x60
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0x4000049300)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0xe8
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xb4

goroutine 12484 [runnable]:
reflect.(*rtype).ptrTo(0xfcf4c0, 0x40003ec1c0)
	/usr/local/go/src/reflect/type.go:1382 +0x354
reflect.New(0x12b57a8, 0xfcf4c0, 0x40006cd888, 0x400043cdb8, 0x40003fa001)
	/usr/local/go/src/reflect/value.go:2403 +0x58
k8s.io/apimachinery/pkg/runtime.(*Scheme).New(0x400037c7e0, 0x0, 0x0, 0x400052c118, 0x2, 0x4000628030, 0x15, 0x7976b8, 0xfecaa0, 0x40003fa060, ...)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/scheme.go:299 +0x210
k8s.io/apimachinery/pkg/runtime.UseOrCreateObject(0x126f8c0, 0x400037c7e0, 0x1267248, 0x400037c7e0, 0x0, 0x0, 0x400052c118, 0x2, 0x4000628030, 0x15, ...)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/codec.go:97 +0x5c
k8s.io/apimachinery/pkg/runtime/serializer/json.(*Serializer).Decode(0x400014ea00, 0x40003e00a0, 0x96, 0xa0, 0x0, 0x0, 0x0, 0x40001a8d50, 0x40006cddb8, 0xbde320, ...)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/json/json.go:259 +0x3b0
k8s.io/apimachinery/pkg/runtime.WithoutVersionDecoder.Decode(0xffff68cd7118, 0x400014ea00, 0x40003e00a0, 0x96, 0xa0, 0x0, 0x0, 0x0, 0x40006cddb8, 0xbde2ec, ...)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/helper.go:252 +0x60
k8s.io/apimachinery/pkg/runtime.Decode(...)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/codec.go:58
k8s.io/client-go/rest/watch.(*Decoder).Decode(0x4000811060, 0x40006cdef0, 0x8, 0x126e088, 0x4000164540, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:62 +0x148
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0x400053b1c0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0xe8
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xb4

